{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "# Useful starting lines\r\n",
    "%matplotlib inline\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "%load_ext autoreload\r\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import datetime\r\n",
    "from helpers import *\r\n",
    "\r\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\r\n",
    "x, mean_x, std_x = standardize(height)\r\n",
    "y, tx = build_model_data(x, weight)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "y.shape, tx.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1 Computing the Cost Function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fill in the `compute_loss` function below:\r\n",
    "<a id='compute_loss'></a>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def compute_loss(y, tx, w, mae=False):\r\n",
    "    \"\"\"Calculate the loss.\r\n",
    "\r\n",
    "    You can calculate the loss using mse or mae.\r\n",
    "    \"\"\"\r\n",
    "    preds = tx.dot(w)\r\n",
    "    if mae:\r\n",
    "        return mae(y, preds)\r\n",
    "    else:\r\n",
    "        return mse(y, preds)\r\n",
    "\r\n",
    "def mse(y, preds):\r\n",
    "    \"\"\"\r\n",
    "    :param y: True values of y.\r\n",
    "    :param tx: Input values of x.\r\n",
    "    :param w: Weights.\r\n",
    "    :return: Mean square error between the predicted and true labels.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    return 1/2 * np.mean((y - preds)**2)\r\n",
    "\r\n",
    "def mae(y, preds):\r\n",
    "    \"\"\"\r\n",
    "    :param y: True values of y.\r\n",
    "    :param tx: Input values of x.\r\n",
    "    :param w: Weights.\r\n",
    "    :return: Mean absolute error between the predicted and true labels.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    return np.mean(np.abs(y - preds))\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Grid Search\n",
    "\n",
    "Fill in the function `grid_search()` below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "def grid_search(y, tx, w0, w1):\r\n",
    "    \"\"\"Algorithm for grid search.\"\"\"\r\n",
    "    losses = np.zeros((len(w0), len(w1)))\r\n",
    "    # INSERT YOUR CODE HERE\r\n",
    "    for i, w0_i in enumerate(w0):\r\n",
    "        for j, w1_j in enumerate(w1):\r\n",
    "            losses[i][j] = compute_loss(y, tx, np.array([w0_i, w1_j]))\r\n",
    "\r\n",
    "    return losses"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us play with the grid search demo now!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "from grid_search import generate_w, get_best_parameters\r\n",
    "from plots import grid_visualization\r\n",
    "\r\n",
    "# Generate the grid of parameters to be swept\r\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\r\n",
    "\r\n",
    "# Start the grid search\r\n",
    "start_time = datetime.datetime.now()\r\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\r\n",
    "\r\n",
    "# Select the best combinaison\r\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\r\n",
    "end_time = datetime.datetime.now()\r\n",
    "execution_time = (end_time - start_time).total_seconds()\r\n",
    "\r\n",
    "# Print the results\r\n",
    "print(\"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\r\n",
    "      l=loss_star, w0=w0_star, w1=w1_star, t=execution_time))\r\n",
    "\r\n",
    "# Plot the results\r\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\r\n",
    "fig.set_size_inches(10.0,6.0)\r\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Grid Search: loss*=7.956595728172087, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.004 seconds\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Gradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "def compute_gradient(y, tx, w, subgradient = False):\r\n",
    "    \"\"\"Compute the gradient.\"\"\"\r\n",
    "    if subgradient:\r\n",
    "        return compute_subgradient_mae(y, tx, w)\r\n",
    "    else:\r\n",
    "        return compute_gradient_mse(y, tx, w)\r\n",
    "\r\n",
    "def compute_gradient_mse(y, tx, w):\r\n",
    "    pred = np.dot(tx, w)\r\n",
    "    e = y - pred\r\n",
    "    N = e.shape[0]\r\n",
    "    gradient = -1/N * np.matmul(tx.T, e)\r\n",
    "    return gradient\r\n",
    "\r\n",
    "\r\n",
    "def compute_subgradient_mae(y, tx, w):\r\n",
    "    e = y - tx.dot(w)\r\n",
    "    N = len(e)\r\n",
    "    grad = -1/N * tx.T @ np.sign(e)\r\n",
    "    return grad"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\r\n",
    "    \"\"\"Gradient descent algorithm.\"\"\"\r\n",
    "    # Define parameters to store w and loss\r\n",
    "    ws = [initial_w]\r\n",
    "    losses = []\r\n",
    "    w = initial_w\r\n",
    "    for n_iter in range(max_iters):\r\n",
    "        # ***************************************************\r\n",
    "        mae = True\r\n",
    "        loss = compute_loss(y, tx, w, mae=mae)\r\n",
    "        gradient = compute_gradient(y, tx, w, subgradient=mae)\r\n",
    "        # ***************************************************\r\n",
    "\r\n",
    "        # ***************************************************\r\n",
    "        w = w - gamma * gradient\r\n",
    "        # ***************************************************\r\n",
    "\r\n",
    "        # store w and loss\r\n",
    "        ws.append(w)\r\n",
    "        losses.append(loss)\r\n",
    "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\r\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\r\n",
    "\r\n",
    "    return losses, ws"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "# from gradient_descent import *\r\n",
    "from plots import gradient_descent_visualization\r\n",
    "\r\n",
    "# Define the parameters of the algorithm.\r\n",
    "max_iters = 500\r\n",
    "gamma = 0.25\r\n",
    "\r\n",
    "# Initialization\r\n",
    "w_initial = np.array([0, 0])\r\n",
    "\r\n",
    "# Start gradient descent.\r\n",
    "start_time = datetime.datetime.now()\r\n",
    "gradient_losses, gradient_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\r\n",
    "end_time = datetime.datetime.now()\r\n",
    "\r\n",
    "# Print result\r\n",
    "exection_time = (end_time - start_time).total_seconds()\r\n",
    "print(\"Gradient Descent: execution time={t:.3f} seconds\".format(t=exection_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Gradient Descent(0/499): loss=74.06780585492638, w0=0.25000000000000017, w1=2.723515857283587e-16\n",
      "Gradient Descent(1/499): loss=73.81780585492638, w0=0.5000000000000003, w1=5.447031714567174e-16\n",
      "Gradient Descent(2/499): loss=73.56780585492638, w0=0.7500000000000004, w1=8.170547571850761e-16\n",
      "Gradient Descent(3/499): loss=73.31780585492638, w0=1.0000000000000007, w1=1.0894063429134349e-15\n",
      "Gradient Descent(4/499): loss=73.06780585492638, w0=1.2500000000000009, w1=1.3617579286417936e-15\n",
      "Gradient Descent(5/499): loss=72.81780585492638, w0=1.500000000000001, w1=1.6341095143701523e-15\n",
      "Gradient Descent(6/499): loss=72.56780585492638, w0=1.7500000000000013, w1=1.906461100098511e-15\n",
      "Gradient Descent(7/499): loss=72.31780585492638, w0=2.0000000000000013, w1=2.1788126858268697e-15\n",
      "Gradient Descent(8/499): loss=72.06780585492638, w0=2.2500000000000013, w1=2.4511642715552284e-15\n",
      "Gradient Descent(9/499): loss=71.81780585492638, w0=2.5000000000000013, w1=2.723515857283587e-15\n",
      "Gradient Descent(10/499): loss=71.56780585492638, w0=2.7500000000000013, w1=2.995867443011946e-15\n",
      "Gradient Descent(11/499): loss=71.31780585492638, w0=3.0000000000000013, w1=3.2682190287403046e-15\n",
      "Gradient Descent(12/499): loss=71.06780585492638, w0=3.2500000000000013, w1=3.5405706144686633e-15\n",
      "Gradient Descent(13/499): loss=70.81780585492638, w0=3.5000000000000013, w1=3.812922200197022e-15\n",
      "Gradient Descent(14/499): loss=70.56780585492638, w0=3.7500000000000013, w1=4.085273785925381e-15\n",
      "Gradient Descent(15/499): loss=70.31780585492638, w0=4.000000000000002, w1=4.3576253716537394e-15\n",
      "Gradient Descent(16/499): loss=70.06780585492638, w0=4.250000000000002, w1=4.629976957382098e-15\n",
      "Gradient Descent(17/499): loss=69.81780585492638, w0=4.500000000000002, w1=4.902328543110457e-15\n",
      "Gradient Descent(18/499): loss=69.56780585492638, w0=4.750000000000002, w1=5.1746801288388156e-15\n",
      "Gradient Descent(19/499): loss=69.31780585492638, w0=5.000000000000002, w1=5.447031714567174e-15\n",
      "Gradient Descent(20/499): loss=69.06780585492638, w0=5.250000000000002, w1=5.719383300295533e-15\n",
      "Gradient Descent(21/499): loss=68.81780585492638, w0=5.500000000000002, w1=5.991734886023892e-15\n",
      "Gradient Descent(22/499): loss=68.56780585492638, w0=5.750000000000002, w1=6.2640864717522504e-15\n",
      "Gradient Descent(23/499): loss=68.31780585492638, w0=6.000000000000002, w1=6.536438057480609e-15\n",
      "Gradient Descent(24/499): loss=68.06780585492638, w0=6.250000000000002, w1=6.808789643208968e-15\n",
      "Gradient Descent(25/499): loss=67.81780585492638, w0=6.500000000000002, w1=7.0811412289373266e-15\n",
      "Gradient Descent(26/499): loss=67.56780585492638, w0=6.750000000000002, w1=7.353492814665685e-15\n",
      "Gradient Descent(27/499): loss=67.31780585492638, w0=7.000000000000002, w1=7.625844400394044e-15\n",
      "Gradient Descent(28/499): loss=67.06780585492638, w0=7.250000000000002, w1=7.898195986122403e-15\n",
      "Gradient Descent(29/499): loss=66.81780585492638, w0=7.500000000000002, w1=8.170547571850761e-15\n",
      "Gradient Descent(30/499): loss=66.56780585492638, w0=7.750000000000002, w1=8.44289915757912e-15\n",
      "Gradient Descent(31/499): loss=66.31780585492638, w0=8.000000000000002, w1=8.715250743307479e-15\n",
      "Gradient Descent(32/499): loss=66.06780585492638, w0=8.250000000000002, w1=8.987602329035838e-15\n",
      "Gradient Descent(33/499): loss=65.81780585492638, w0=8.500000000000002, w1=9.259953914764196e-15\n",
      "Gradient Descent(34/499): loss=65.56780585492638, w0=8.750000000000002, w1=9.532305500492555e-15\n",
      "Gradient Descent(35/499): loss=65.31780585492638, w0=9.000000000000002, w1=9.804657086220914e-15\n",
      "Gradient Descent(36/499): loss=65.06780585492638, w0=9.250000000000002, w1=1.0077008671949272e-14\n",
      "Gradient Descent(37/499): loss=64.81780585492638, w0=9.500000000000002, w1=1.0349360257677631e-14\n",
      "Gradient Descent(38/499): loss=64.56780585492638, w0=9.750000000000002, w1=1.062171184340599e-14\n",
      "Gradient Descent(39/499): loss=64.31780585492638, w0=10.000000000000002, w1=1.0894063429134349e-14\n",
      "Gradient Descent(40/499): loss=64.06780585492638, w0=10.250000000000002, w1=1.1166415014862707e-14\n",
      "Gradient Descent(41/499): loss=63.817805854926384, w0=10.500000000000002, w1=1.1438766600591066e-14\n",
      "Gradient Descent(42/499): loss=63.567805854926384, w0=10.750000000000002, w1=1.1711118186319425e-14\n",
      "Gradient Descent(43/499): loss=63.317805854926384, w0=11.000000000000002, w1=1.1983469772047783e-14\n",
      "Gradient Descent(44/499): loss=63.067805854926384, w0=11.250000000000002, w1=1.2255821357776142e-14\n",
      "Gradient Descent(45/499): loss=62.817805854926384, w0=11.500000000000002, w1=1.2528172943504501e-14\n",
      "Gradient Descent(46/499): loss=62.567805854926384, w0=11.750000000000002, w1=1.280052452923286e-14\n",
      "Gradient Descent(47/499): loss=62.317805854926384, w0=12.000000000000002, w1=1.3072876114961218e-14\n",
      "Gradient Descent(48/499): loss=62.067805854926384, w0=12.250000000000002, w1=1.3345227700689577e-14\n",
      "Gradient Descent(49/499): loss=61.817805854926384, w0=12.500000000000002, w1=1.3617579286417936e-14\n",
      "Gradient Descent(50/499): loss=61.567805854926384, w0=12.750000000000002, w1=1.3889930872146294e-14\n",
      "Gradient Descent(51/499): loss=61.317805854926384, w0=13.000000000000002, w1=1.4162282457874653e-14\n",
      "Gradient Descent(52/499): loss=61.067805854926384, w0=13.250000000000002, w1=1.4434634043603012e-14\n",
      "Gradient Descent(53/499): loss=60.817805854926384, w0=13.500000000000002, w1=1.470698562933137e-14\n",
      "Gradient Descent(54/499): loss=60.567805854926384, w0=13.750000000000002, w1=1.497933721505973e-14\n",
      "Gradient Descent(55/499): loss=60.317805854926384, w0=14.000000000000002, w1=1.5251688800788088e-14\n",
      "Gradient Descent(56/499): loss=60.067805854926384, w0=14.250000000000002, w1=1.5524040386516447e-14\n",
      "Gradient Descent(57/499): loss=59.817805854926384, w0=14.500000000000002, w1=1.5796391972244805e-14\n",
      "Gradient Descent(58/499): loss=59.567805854926384, w0=14.750000000000002, w1=1.6068743557973164e-14\n",
      "Gradient Descent(59/499): loss=59.317805854926384, w0=15.000000000000002, w1=1.6341095143701523e-14\n",
      "Gradient Descent(60/499): loss=59.067805854926384, w0=15.250000000000002, w1=1.661344672942988e-14\n",
      "Gradient Descent(61/499): loss=58.817805854926384, w0=15.500000000000002, w1=1.688579831515824e-14\n",
      "Gradient Descent(62/499): loss=58.567805854926384, w0=15.750000000000002, w1=1.71581499008866e-14\n",
      "Gradient Descent(63/499): loss=58.317805854926384, w0=16.000000000000004, w1=1.7430501486614958e-14\n",
      "Gradient Descent(64/499): loss=58.067805854926384, w0=16.250000000000004, w1=1.7702853072343316e-14\n",
      "Gradient Descent(65/499): loss=57.817805854926384, w0=16.500000000000004, w1=1.7975204658071675e-14\n",
      "Gradient Descent(66/499): loss=57.567805854926384, w0=16.750000000000004, w1=1.8247556243800034e-14\n",
      "Gradient Descent(67/499): loss=57.317805854926384, w0=17.000000000000004, w1=1.8519907829528393e-14\n",
      "Gradient Descent(68/499): loss=57.067805854926384, w0=17.250000000000004, w1=1.879225941525675e-14\n",
      "Gradient Descent(69/499): loss=56.817805854926384, w0=17.500000000000004, w1=1.906461100098511e-14\n",
      "Gradient Descent(70/499): loss=56.567805854926384, w0=17.750000000000004, w1=1.933696258671347e-14\n",
      "Gradient Descent(71/499): loss=56.317805854926384, w0=18.000000000000004, w1=1.9609314172441827e-14\n",
      "Gradient Descent(72/499): loss=56.067805854926384, w0=18.250000000000004, w1=1.9881665758170186e-14\n",
      "Gradient Descent(73/499): loss=55.817805854926384, w0=18.500000000000004, w1=2.0154017343898545e-14\n",
      "Gradient Descent(74/499): loss=55.567805854926384, w0=18.750000000000004, w1=2.0426368929626904e-14\n",
      "Gradient Descent(75/499): loss=55.317805854926384, w0=19.000000000000004, w1=2.0698720515355262e-14\n",
      "Gradient Descent(76/499): loss=55.067805854926384, w0=19.250000000000004, w1=2.097107210108362e-14\n",
      "Gradient Descent(77/499): loss=54.817805854926384, w0=19.500000000000004, w1=2.124342368681198e-14\n",
      "Gradient Descent(78/499): loss=54.567805854926384, w0=19.750000000000004, w1=2.1515775272540338e-14\n",
      "Gradient Descent(79/499): loss=54.317805854926384, w0=20.000000000000004, w1=2.1788126858268697e-14\n",
      "Gradient Descent(80/499): loss=54.067805854926384, w0=20.250000000000004, w1=2.2060478443997056e-14\n",
      "Gradient Descent(81/499): loss=53.81780585492638, w0=20.500000000000004, w1=2.2332830029725415e-14\n",
      "Gradient Descent(82/499): loss=53.56780585492638, w0=20.750000000000004, w1=2.2605181615453773e-14\n",
      "Gradient Descent(83/499): loss=53.31780585492638, w0=21.000000000000004, w1=2.2877533201182132e-14\n",
      "Gradient Descent(84/499): loss=53.06780585492638, w0=21.250000000000004, w1=2.314988478691049e-14\n",
      "Gradient Descent(85/499): loss=52.81780585492638, w0=21.500000000000004, w1=2.342223637263885e-14\n",
      "Gradient Descent(86/499): loss=52.56780585492638, w0=21.750000000000004, w1=2.3694587958367208e-14\n",
      "Gradient Descent(87/499): loss=52.31780585492638, w0=22.000000000000004, w1=2.3966939544095567e-14\n",
      "Gradient Descent(88/499): loss=52.06780585492638, w0=22.250000000000004, w1=2.4239291129823926e-14\n",
      "Gradient Descent(89/499): loss=51.81780585492638, w0=22.500000000000004, w1=2.4511642715552284e-14\n",
      "Gradient Descent(90/499): loss=51.56780585492638, w0=22.750000000000004, w1=2.4783994301280643e-14\n",
      "Gradient Descent(91/499): loss=51.31780585492638, w0=23.000000000000004, w1=2.5056345887009002e-14\n",
      "Gradient Descent(92/499): loss=51.06780585492638, w0=23.250000000000004, w1=2.532869747273736e-14\n",
      "Gradient Descent(93/499): loss=50.817805854926384, w0=23.500000000000004, w1=2.560104905846572e-14\n",
      "Gradient Descent(94/499): loss=50.56780585492638, w0=23.750000000000004, w1=2.5873400644194078e-14\n",
      "Gradient Descent(95/499): loss=50.31780585492638, w0=24.000000000000004, w1=2.6145752229922437e-14\n",
      "Gradient Descent(96/499): loss=50.06780585492638, w0=24.250000000000004, w1=2.6418103815650795e-14\n",
      "Gradient Descent(97/499): loss=49.81780585492638, w0=24.500000000000004, w1=2.6690455401379154e-14\n",
      "Gradient Descent(98/499): loss=49.56780585492638, w0=24.750000000000004, w1=2.6962806987107513e-14\n",
      "Gradient Descent(99/499): loss=49.31780585492638, w0=25.000000000000004, w1=2.723515857283587e-14\n",
      "Gradient Descent(100/499): loss=49.06780585492638, w0=25.250000000000004, w1=2.750751015856423e-14\n",
      "Gradient Descent(101/499): loss=48.81780585492638, w0=25.500000000000004, w1=2.777986174429259e-14\n",
      "Gradient Descent(102/499): loss=48.56780585492638, w0=25.750000000000004, w1=2.8052213330020948e-14\n",
      "Gradient Descent(103/499): loss=48.31780585492638, w0=26.000000000000004, w1=2.8324564915749306e-14\n",
      "Gradient Descent(104/499): loss=48.06780585492638, w0=26.250000000000004, w1=2.8596916501477665e-14\n",
      "Gradient Descent(105/499): loss=47.81780585492638, w0=26.500000000000004, w1=2.8869268087206024e-14\n",
      "Gradient Descent(106/499): loss=47.56780585492638, w0=26.750000000000004, w1=2.914161967293438e-14\n",
      "Gradient Descent(107/499): loss=47.31780585492638, w0=27.000000000000004, w1=2.941397125866274e-14\n",
      "Gradient Descent(108/499): loss=47.06780585492638, w0=27.250000000000004, w1=2.96863228443911e-14\n",
      "Gradient Descent(109/499): loss=46.81780585492638, w0=27.500000000000004, w1=2.995867443011946e-14\n",
      "Gradient Descent(110/499): loss=46.56780585492638, w0=27.750000000000004, w1=3.023102601584782e-14\n",
      "Gradient Descent(111/499): loss=46.31780585492638, w0=28.000000000000004, w1=3.0503377601576176e-14\n",
      "Gradient Descent(112/499): loss=46.06780585492638, w0=28.250000000000004, w1=3.0775729187304535e-14\n",
      "Gradient Descent(113/499): loss=45.81780585492638, w0=28.500000000000004, w1=3.1048080773032893e-14\n",
      "Gradient Descent(114/499): loss=45.56780585492638, w0=28.750000000000004, w1=3.132043235876125e-14\n",
      "Gradient Descent(115/499): loss=45.31780585492638, w0=29.000000000000004, w1=3.159278394448961e-14\n",
      "Gradient Descent(116/499): loss=45.06780585492638, w0=29.250000000000004, w1=3.186513553021797e-14\n",
      "Gradient Descent(117/499): loss=44.81780585492638, w0=29.500000000000004, w1=3.213748711594633e-14\n",
      "Gradient Descent(118/499): loss=44.56780585492638, w0=29.750000000000004, w1=3.240983870167469e-14\n",
      "Gradient Descent(119/499): loss=44.31780585492638, w0=30.000000000000004, w1=3.2682190287403046e-14\n",
      "Gradient Descent(120/499): loss=44.06780585492638, w0=30.250000000000004, w1=3.2954541873131404e-14\n",
      "Gradient Descent(121/499): loss=43.81780585492638, w0=30.500000000000004, w1=3.322689345885976e-14\n",
      "Gradient Descent(122/499): loss=43.56780585492638, w0=30.750000000000004, w1=3.349924504458812e-14\n",
      "Gradient Descent(123/499): loss=43.31780585492638, w0=31.000000000000004, w1=3.377159663031648e-14\n",
      "Gradient Descent(124/499): loss=43.06780585492638, w0=31.250000000000004, w1=3.404394821604484e-14\n",
      "Gradient Descent(125/499): loss=42.81780585492638, w0=31.500000000000004, w1=3.43162998017732e-14\n",
      "Gradient Descent(126/499): loss=42.56780585492638, w0=31.750000000000004, w1=3.4588651387501557e-14\n",
      "Gradient Descent(127/499): loss=42.31780585492638, w0=32.00000000000001, w1=3.4861002973229915e-14\n",
      "Gradient Descent(128/499): loss=42.06780585492638, w0=32.25000000000001, w1=3.5133354558958274e-14\n",
      "Gradient Descent(129/499): loss=41.81780585492638, w0=32.50000000000001, w1=3.540570614468663e-14\n",
      "Gradient Descent(130/499): loss=41.56780585492638, w0=32.75000000000001, w1=3.567805773041499e-14\n",
      "Gradient Descent(131/499): loss=41.31780585492638, w0=33.00000000000001, w1=3.595040931614335e-14\n",
      "Gradient Descent(132/499): loss=41.06780585492638, w0=33.25000000000001, w1=3.622276090187171e-14\n",
      "Gradient Descent(133/499): loss=40.81780585492638, w0=33.50000000000001, w1=3.649511248760007e-14\n",
      "Gradient Descent(134/499): loss=40.56780585492638, w0=33.75000000000001, w1=3.6767464073328426e-14\n",
      "Gradient Descent(135/499): loss=40.31780585492638, w0=34.00000000000001, w1=3.7039815659056785e-14\n",
      "Gradient Descent(136/499): loss=40.06780585492638, w0=34.25000000000001, w1=3.7312167244785144e-14\n",
      "Gradient Descent(137/499): loss=39.81780585492638, w0=34.50000000000001, w1=3.75845188305135e-14\n",
      "Gradient Descent(138/499): loss=39.56780585492638, w0=34.75000000000001, w1=3.785687041624186e-14\n",
      "Gradient Descent(139/499): loss=39.31780585492638, w0=35.00000000000001, w1=3.812922200197022e-14\n",
      "Gradient Descent(140/499): loss=39.06780585492638, w0=35.25000000000001, w1=3.840157358769858e-14\n",
      "Gradient Descent(141/499): loss=38.81780585492638, w0=35.50000000000001, w1=3.867392517342694e-14\n",
      "Gradient Descent(142/499): loss=38.56780585492638, w0=35.75000000000001, w1=3.8946276759155296e-14\n",
      "Gradient Descent(143/499): loss=38.31780585492638, w0=36.00000000000001, w1=3.9218628344883655e-14\n",
      "Gradient Descent(144/499): loss=38.06780585492638, w0=36.25000000000001, w1=3.9490979930612014e-14\n",
      "Gradient Descent(145/499): loss=37.81780585492638, w0=36.50000000000001, w1=3.976333151634037e-14\n",
      "Gradient Descent(146/499): loss=37.56780585492638, w0=36.75000000000001, w1=4.003568310206873e-14\n",
      "Gradient Descent(147/499): loss=37.31780585492638, w0=37.00000000000001, w1=4.030803468779709e-14\n",
      "Gradient Descent(148/499): loss=37.06780585492638, w0=37.25000000000001, w1=4.058038627352545e-14\n",
      "Gradient Descent(149/499): loss=36.81780585492638, w0=37.50000000000001, w1=4.085273785925381e-14\n",
      "Gradient Descent(150/499): loss=36.56780585492638, w0=37.75000000000001, w1=4.1125089444982166e-14\n",
      "Gradient Descent(151/499): loss=36.31780585492638, w0=38.00000000000001, w1=4.1397441030710525e-14\n",
      "Gradient Descent(152/499): loss=36.06780585492638, w0=38.25000000000001, w1=4.1669792616438883e-14\n",
      "Gradient Descent(153/499): loss=35.81780585492638, w0=38.50000000000001, w1=4.194214420216724e-14\n",
      "Gradient Descent(154/499): loss=35.56780585492638, w0=38.75000000000001, w1=4.22144957878956e-14\n",
      "Gradient Descent(155/499): loss=35.31780585492637, w0=39.00000000000001, w1=4.248684737362396e-14\n",
      "Gradient Descent(156/499): loss=35.06780585492637, w0=39.25000000000001, w1=4.275919895935232e-14\n",
      "Gradient Descent(157/499): loss=34.81780585492637, w0=39.50000000000001, w1=4.3031550545080677e-14\n",
      "Gradient Descent(158/499): loss=34.56780585492637, w0=39.75000000000001, w1=4.3303902130809035e-14\n",
      "Gradient Descent(159/499): loss=34.31780585492637, w0=40.00000000000001, w1=4.3576253716537394e-14\n",
      "Gradient Descent(160/499): loss=34.06780585492637, w0=40.25000000000001, w1=4.384860530226575e-14\n",
      "Gradient Descent(161/499): loss=33.81780585492637, w0=40.50000000000001, w1=4.412095688799411e-14\n",
      "Gradient Descent(162/499): loss=33.56780585492637, w0=40.75000000000001, w1=4.439330847372247e-14\n",
      "Gradient Descent(163/499): loss=33.31780585492637, w0=41.00000000000001, w1=4.466566005945083e-14\n",
      "Gradient Descent(164/499): loss=33.06780585492637, w0=41.25000000000001, w1=4.493801164517919e-14\n",
      "Gradient Descent(165/499): loss=32.81780585492637, w0=41.50000000000001, w1=4.5210363230907546e-14\n",
      "Gradient Descent(166/499): loss=32.56780585492637, w0=41.75000000000001, w1=4.5482714816635905e-14\n",
      "Gradient Descent(167/499): loss=32.31780585492637, w0=42.00000000000001, w1=4.5755066402364264e-14\n",
      "Gradient Descent(168/499): loss=32.06780585492637, w0=42.25000000000001, w1=4.602741798809262e-14\n",
      "Gradient Descent(169/499): loss=31.81780585492637, w0=42.50000000000001, w1=4.629976957382098e-14\n",
      "Gradient Descent(170/499): loss=31.56780585492637, w0=42.75000000000001, w1=4.657212115954934e-14\n",
      "Gradient Descent(171/499): loss=31.317805854926373, w0=43.00000000000001, w1=4.68444727452777e-14\n",
      "Gradient Descent(172/499): loss=31.067805854926373, w0=43.25000000000001, w1=4.711682433100606e-14\n",
      "Gradient Descent(173/499): loss=30.817805854926373, w0=43.50000000000001, w1=4.7389175916734416e-14\n",
      "Gradient Descent(174/499): loss=30.56780585492637, w0=43.75000000000001, w1=4.7661527502462775e-14\n",
      "Gradient Descent(175/499): loss=30.317805854926366, w0=44.00000000000001, w1=4.7933879088191134e-14\n",
      "Gradient Descent(176/499): loss=30.06780585492637, w0=44.25000000000001, w1=4.820623067391949e-14\n",
      "Gradient Descent(177/499): loss=29.81780585492637, w0=44.50000000000001, w1=4.847858225964785e-14\n",
      "Gradient Descent(178/499): loss=29.56780585492637, w0=44.75000000000001, w1=4.875093384537621e-14\n",
      "Gradient Descent(179/499): loss=29.31780585492637, w0=45.00000000000001, w1=4.902328543110457e-14\n",
      "Gradient Descent(180/499): loss=29.06780585492637, w0=45.25000000000001, w1=4.929563701683293e-14\n",
      "Gradient Descent(181/499): loss=28.817805854926366, w0=45.50000000000001, w1=4.9567988602561286e-14\n",
      "Gradient Descent(182/499): loss=28.56780585492637, w0=45.75000000000001, w1=4.9840340188289645e-14\n",
      "Gradient Descent(183/499): loss=28.31780585492637, w0=46.00000000000001, w1=5.0112691774018003e-14\n",
      "Gradient Descent(184/499): loss=28.06780585492637, w0=46.25000000000001, w1=5.038504335974636e-14\n",
      "Gradient Descent(185/499): loss=27.817805854926373, w0=46.50000000000001, w1=5.065739494547472e-14\n",
      "Gradient Descent(186/499): loss=27.56930981362935, w0=46.74752475247526, w1=0.003981373456561434\n",
      "Gradient Descent(187/499): loss=27.324172395939215, w0=46.99504950495051, w1=0.007962746913072211\n",
      "Gradient Descent(188/499): loss=27.079034978249076, w0=47.24257425742576, w1=0.011944120369582988\n",
      "Gradient Descent(189/499): loss=26.83398116209652, w0=47.48762376237625, w1=0.019779083016593126\n",
      "Gradient Descent(190/499): loss=26.59353857603187, w0=47.732673267326746, w1=0.027614045663603266\n",
      "Gradient Descent(191/499): loss=26.353095989967223, w0=47.97772277227724, w1=0.03544900831061341\n",
      "Gradient Descent(192/499): loss=26.112653403902574, w0=48.22277227722773, w1=0.04328397095762355\n",
      "Gradient Descent(193/499): loss=25.872210817837924, w0=48.46782178217823, w1=0.05111893360463369\n",
      "Gradient Descent(194/499): loss=25.631768231773275, w0=48.71287128712872, w1=0.05895389625164383\n",
      "Gradient Descent(195/499): loss=25.395358206430707, w0=48.95297029702971, w1=0.07398318729925267\n",
      "Gradient Descent(196/499): loss=25.165438993087236, w0=49.190594059405946, w1=0.09196876850694927\n",
      "Gradient Descent(197/499): loss=24.939247648665855, w0=49.425742574257434, w1=0.11414106188011225\n",
      "Gradient Descent(198/499): loss=24.716101910144715, w0=49.66089108910892, w1=0.13631335525327523\n",
      "Gradient Descent(199/499): loss=24.49295617162357, w0=49.89603960396041, w1=0.1584856486264382\n",
      "Gradient Descent(200/499): loss=24.269810433102432, w0=50.1311881188119, w1=0.1806579419996012\n",
      "Gradient Descent(201/499): loss=24.046664694581292, w0=50.36633663366339, w1=0.20283023537276418\n",
      "Gradient Descent(202/499): loss=23.82358036541574, w0=50.59900990099012, w1=0.22928620232416683\n",
      "Gradient Descent(203/499): loss=23.60533364876373, w0=50.8292079207921, w1=0.25888513895084\n",
      "Gradient Descent(204/499): loss=23.389864747283, w0=51.05940594059408, w1=0.2884840755775131\n",
      "Gradient Descent(205/499): loss=23.17439584580226, w0=51.289603960396065, w1=0.3180830122041862\n",
      "Gradient Descent(206/499): loss=22.959967606527428, w0=51.51732673267329, w1=0.3506572618072594\n",
      "Gradient Descent(207/499): loss=22.748791894083478, w0=51.742574257425765, w1=0.3862019059345894\n",
      "Gradient Descent(208/499): loss=22.540792417550456, w0=51.96782178217824, w1=0.4217465500619194\n",
      "Gradient Descent(209/499): loss=22.332792941017434, w0=52.19306930693072, w1=0.4572911941892494\n",
      "Gradient Descent(210/499): loss=22.12568394934845, w0=52.41584158415844, w1=0.4961721412993839\n",
      "Gradient Descent(211/499): loss=21.921127087150822, w0=52.63861386138617, w1=0.5350530884095185\n",
      "Gradient Descent(212/499): loss=21.717450073733016, w0=52.85891089108914, w1=0.5767915805728423\n",
      "Gradient Descent(213/499): loss=21.51704220673963, w0=53.07673267326736, w1=0.6214249043125651\n",
      "Gradient Descent(214/499): loss=21.31928835722223, w0=53.29455445544558, w1=0.666058228052288\n",
      "Gradient Descent(215/499): loss=21.121905070770588, w0=53.50990099009904, w1=0.713879784693046\n",
      "Gradient Descent(216/499): loss=20.927260945703384, w0=53.725247524752504, w1=0.7617013413338041\n",
      "Gradient Descent(217/499): loss=20.733691397189325, w0=53.93811881188122, w1=0.811725112991607\n",
      "Gradient Descent(218/499): loss=20.542425146730494, w0=54.15099009900993, w1=0.8617488846494099\n",
      "Gradient Descent(219/499): loss=20.35115889627167, w0=54.363861386138645, w1=0.9117726563072128\n",
      "Gradient Descent(220/499): loss=20.159892645812842, w0=54.57673267326736, w1=0.9617964279650157\n",
      "Gradient Descent(221/499): loss=19.969305266806497, w0=54.78712871287132, w1=1.0144160777098095\n",
      "Gradient Descent(222/499): loss=19.781163982725314, w0=54.997524752475286, w1=1.0670357274546032\n",
      "Gradient Descent(223/499): loss=19.5942131786318, w0=55.20544554455449, w1=1.122143018353537\n",
      "Gradient Descent(224/499): loss=19.409276018013305, w0=55.41089108910895, w1=1.178940720152139\n",
      "Gradient Descent(225/499): loss=19.227540615185784, w0=55.61633663366341, w1=1.2357384219507408\n",
      "Gradient Descent(226/499): loss=19.046927665938135, w0=55.81683168316836, w1=1.297516993824397\n",
      "Gradient Descent(227/499): loss=18.871611947180572, w0=56.014851485148554, w1=1.3612303687469707\n",
      "Gradient Descent(228/499): loss=18.700178080719915, w0=56.20792079207925, w1=1.4291266991276772\n",
      "Gradient Descent(229/499): loss=18.53263540488846, w0=56.400990099009945, w1=1.4970230295083837\n",
      "Gradient Descent(230/499): loss=18.36623402022168, w0=56.591584158415884, w1=1.5667174543648323\n",
      "Gradient Descent(231/499): loss=18.201500386874052, w0=56.78217821782182, w1=1.636411879221281\n",
      "Gradient Descent(232/499): loss=18.03676675352643, w0=56.97277227722776, w1=1.7061063040777296\n",
      "Gradient Descent(233/499): loss=17.87203312017881, w0=57.1633663366337, w1=1.7758007289341782\n",
      "Gradient Descent(234/499): loss=17.70820436573979, w0=57.35148514851489, w1=1.848268253960497\n",
      "Gradient Descent(235/499): loss=17.548458567719138, w0=57.53217821782182, w1=1.9267481173381784\n",
      "Gradient Descent(236/499): loss=17.39328113213535, w0=57.710396039604, w1=2.00565192630811\n",
      "Gradient Descent(237/499): loss=17.24133151985236, w0=57.888613861386176, w1=2.0845557352780415\n",
      "Gradient Descent(238/499): loss=17.089381907569372, w0=58.06683168316835, w1=2.163459544247973\n",
      "Gradient Descent(239/499): loss=16.937432295286385, w0=58.24504950495053, w1=2.2423633532179044\n",
      "Gradient Descent(240/499): loss=16.785482683003394, w0=58.423267326732706, w1=2.321267162187836\n",
      "Gradient Descent(241/499): loss=16.635480981741836, w0=58.596534653465376, w1=2.4039591992908838\n",
      "Gradient Descent(242/499): loss=16.489288118311737, w0=58.7673267326733, w1=2.487659632767868\n",
      "Gradient Descent(243/499): loss=16.34458533077414, w0=58.93811881188122, w1=2.5713600662448526\n",
      "Gradient Descent(244/499): loss=16.19990953935801, w0=59.10643564356439, w1=2.65786944365568\n",
      "Gradient Descent(245/499): loss=16.056651826526533, w0=59.27475247524756, w1=2.7443788210665074\n",
      "Gradient Descent(246/499): loss=15.913394113695059, w0=59.44306930693073, w1=2.830888198477335\n",
      "Gradient Descent(247/499): loss=15.771456557586603, w0=59.606435643564396, w1=2.9211445478044724\n",
      "Gradient Descent(248/499): loss=15.633451861637875, w0=59.767326732673304, w1=3.0114495364384544\n",
      "Gradient Descent(249/499): loss=15.498619440234046, w0=59.92326732673271, w1=3.105096862365687\n",
      "Gradient Descent(250/499): loss=15.3673889492392, w0=60.07178217821786, w1=3.2035104016759877\n",
      "Gradient Descent(251/499): loss=15.240421405914256, w0=60.220297029703005, w1=3.3019239409862884\n",
      "Gradient Descent(252/499): loss=15.113453862589315, w0=60.36881188118815, w1=3.400337480296589\n",
      "Gradient Descent(253/499): loss=14.986486319264374, w0=60.5173267326733, w1=3.49875101960689\n",
      "Gradient Descent(254/499): loss=14.860191057012486, w0=60.66336633663369, w1=3.5985269497952954\n",
      "Gradient Descent(255/499): loss=14.735059848333002, w0=60.80940594059409, w1=3.698302879983701\n",
      "Gradient Descent(256/499): loss=14.609928639653521, w0=60.955445544554486, w1=3.7980788101721066\n",
      "Gradient Descent(257/499): loss=14.484797430974037, w0=61.10148514851488, w1=3.897854740360512\n",
      "Gradient Descent(258/499): loss=14.359666222294548, w0=61.24752475247528, w1=3.9976306705489177\n",
      "Gradient Descent(259/499): loss=14.234535013615066, w0=61.393564356435675, w1=4.097406600737323\n",
      "Gradient Descent(260/499): loss=14.109731433227767, w0=61.53712871287132, w1=4.19904588678064\n",
      "Gradient Descent(261/499): loss=13.986017381838566, w0=61.67821782178221, w1=4.301905742441089\n",
      "Gradient Descent(262/499): loss=13.864072235599535, w0=61.8193069306931, w1=4.4047655981015374\n",
      "Gradient Descent(263/499): loss=13.742127089360505, w0=61.96039603960399, w1=4.507625453761986\n",
      "Gradient Descent(264/499): loss=13.620347544589846, w0=62.09900990099013, w1=4.611981912763215\n",
      "Gradient Descent(265/499): loss=13.500457605919916, w0=62.23514851485152, w1=4.7171066072813455\n",
      "Gradient Descent(266/499): loss=13.383358985778798, w0=62.36881188118815, w1=4.822032187177674\n",
      "Gradient Descent(267/499): loss=13.26785789451071, w0=62.502475247524785, w1=4.926957767074003\n",
      "Gradient Descent(268/499): loss=13.152356803242624, w0=62.63613861386142, w1=5.031883346970332\n",
      "Gradient Descent(269/499): loss=13.03731879124268, w0=62.767326732673304, w1=5.138036239404214\n",
      "Gradient Descent(270/499): loss=12.92340375488476, w0=62.89851485148519, w1=5.244189131838096\n",
      "Gradient Descent(271/499): loss=12.809488718526838, w0=63.02970297029707, w1=5.350342024271979\n",
      "Gradient Descent(272/499): loss=12.695573682168918, w0=63.16089108910896, w1=5.456494916705861\n",
      "Gradient Descent(273/499): loss=12.58168605048077, w0=63.289603960396086, w1=5.5648438896686345\n",
      "Gradient Descent(274/499): loss=12.468460037772513, w0=63.418316831683214, w1=5.673192862631408\n",
      "Gradient Descent(275/499): loss=12.355234025064252, w0=63.54702970297034, w1=5.781541835594182\n",
      "Gradient Descent(276/499): loss=12.242008012355992, w0=63.67574257425747, w1=5.889890808556956\n",
      "Gradient Descent(277/499): loss=12.128875040832344, w0=63.80198019801985, w1=5.999238889790898\n",
      "Gradient Descent(278/499): loss=12.017960336651301, w0=63.92574257425747, w1=6.109449866444604\n",
      "Gradient Descent(279/499): loss=11.908105996063512, w0=64.04950495049509, w1=6.21966084309831\n",
      "Gradient Descent(280/499): loss=11.798251655475726, w0=64.17326732673271, w1=6.329871819752015\n",
      "Gradient Descent(281/499): loss=11.688683431644312, w0=64.29455445544558, w1=6.442002939479855\n",
      "Gradient Descent(282/499): loss=11.57954760923299, w0=64.41584158415844, w1=6.554134059207695\n",
      "Gradient Descent(283/499): loss=11.470411786821664, w0=64.53712871287131, w1=6.666265178935535\n",
      "Gradient Descent(284/499): loss=11.361275964410341, w0=64.65841584158417, w1=6.778396298663375\n",
      "Gradient Descent(285/499): loss=11.252140141999016, w0=64.77970297029704, w1=6.890527418391215\n",
      "Gradient Descent(286/499): loss=11.143004319587696, w0=64.9009900990099, w1=7.0026585381190545\n",
      "Gradient Descent(287/499): loss=11.033868497176371, w0=65.02227722772277, w1=7.114789657846894\n",
      "Gradient Descent(288/499): loss=10.924732674765044, w0=65.14356435643563, w1=7.226920777574734\n",
      "Gradient Descent(289/499): loss=10.815596852353723, w0=65.2648514851485, w1=7.339051897302574\n",
      "Gradient Descent(290/499): loss=10.70853519614304, w0=65.38118811881186, w1=7.451804496669169\n",
      "Gradient Descent(291/499): loss=10.603609588377743, w0=65.49504950495047, w1=7.565955387379268\n",
      "Gradient Descent(292/499): loss=10.499630223964484, w0=65.60891089108908, w1=7.680106278089367\n",
      "Gradient Descent(293/499): loss=10.396226572886095, w0=65.72029702970295, w1=7.794900251697539\n",
      "Gradient Descent(294/499): loss=10.29388845987785, w0=65.83168316831681, w1=7.909694225305712\n",
      "Gradient Descent(295/499): loss=10.191616028763606, w0=65.94059405940592, w1=8.026042966965813\n",
      "Gradient Descent(296/499): loss=10.090021581228756, w0=66.04950495049503, w1=8.142391708625913\n",
      "Gradient Descent(297/499): loss=9.988427133693905, w0=66.15841584158414, w1=8.258740450286014\n",
      "Gradient Descent(298/499): loss=9.886832686159055, w0=66.26732673267325, w1=8.375089191946115\n",
      "Gradient Descent(299/499): loss=9.785238238624206, w0=66.37623762376236, w1=8.491437933606216\n",
      "Gradient Descent(300/499): loss=9.683984539209023, w0=66.48267326732672, w1=8.609292283990824\n",
      "Gradient Descent(301/499): loss=9.583428318529547, w0=66.58663366336633, w1=8.728408040027325\n",
      "Gradient Descent(302/499): loss=9.483443009406109, w0=66.69059405940594, w1=8.847523796063827\n",
      "Gradient Descent(303/499): loss=9.383457700282671, w0=66.79455445544555, w1=8.966639552100329\n",
      "Gradient Descent(304/499): loss=9.284296443016183, w0=66.8960396039604, w1=9.086181420412217\n",
      "Gradient Descent(305/499): loss=9.18601134959445, w0=66.9950495049505, w1=9.204801392608893\n",
      "Gradient Descent(306/499): loss=9.090516716402412, w0=67.0940594059406, w1=9.323421364805569\n",
      "Gradient Descent(307/499): loss=8.995022083210374, w0=67.1930693069307, w1=9.442041337002244\n",
      "Gradient Descent(308/499): loss=8.899527450018338, w0=67.29207920792079, w1=9.56066130919892\n",
      "Gradient Descent(309/499): loss=8.8040328168263, w0=67.39108910891089, w1=9.679281281395596\n",
      "Gradient Descent(310/499): loss=8.708538183634264, w0=67.49009900990099, w1=9.797901253592272\n",
      "Gradient Descent(311/499): loss=8.613043550442226, w0=67.58910891089108, w1=9.916521225788948\n",
      "Gradient Descent(312/499): loss=8.518883842275427, w0=67.68811881188118, w1=10.029546429137017\n",
      "Gradient Descent(313/499): loss=8.430495593176834, w0=67.78712871287128, w1=10.137081117068355\n",
      "Gradient Descent(314/499): loss=8.345028914766598, w0=67.88613861386138, w1=10.244615804999693\n",
      "Gradient Descent(315/499): loss=8.259562236356361, w0=67.98514851485147, w1=10.352150492931031\n",
      "Gradient Descent(316/499): loss=8.176105127440238, w0=68.07920792079207, w1=10.4580317855361\n",
      "Gradient Descent(317/499): loss=8.096156418647103, w0=68.17574257425741, w1=10.559629404562928\n",
      "Gradient Descent(318/499): loss=8.017592356600726, w0=68.27227722772275, w1=10.661227023589756\n",
      "Gradient Descent(319/499): loss=7.939843240818019, w0=68.36633663366335, w1=10.762193977406614\n",
      "Gradient Descent(320/499): loss=7.863677250382204, w0=68.46039603960395, w1=10.863160931223472\n",
      "Gradient Descent(321/499): loss=7.78751125994639, w0=68.55445544554455, w1=10.96412788504033\n",
      "Gradient Descent(322/499): loss=7.711345269510578, w0=68.64851485148515, w1=11.065094838857188\n",
      "Gradient Descent(323/499): loss=7.635179279074763, w0=68.74257425742574, w1=11.166061792674046\n",
      "Gradient Descent(324/499): loss=7.559208461229337, w0=68.83415841584159, w1=11.265531075864828\n",
      "Gradient Descent(325/499): loss=7.486081275744432, w0=68.92574257425743, w1=11.36500035905561\n",
      "Gradient Descent(326/499): loss=7.41295409025953, w0=69.01732673267327, w1=11.464469642246392\n",
      "Gradient Descent(327/499): loss=7.339926755609568, w0=69.11138613861387, w1=11.561129981503331\n",
      "Gradient Descent(328/499): loss=7.2677595538598325, w0=69.20049504950495, w1=11.659680340957182\n",
      "Gradient Descent(329/499): loss=7.198739861939367, w0=69.2871287128713, w1=11.754842669679416\n",
      "Gradient Descent(330/499): loss=7.133015899596929, w0=69.37128712871288, w1=11.850714862394044\n",
      "Gradient Descent(331/499): loss=7.067919434425426, w0=69.45544554455446, w1=11.946587055108672\n",
      "Gradient Descent(332/499): loss=7.003693506659656, w0=69.53465346534654, w1=12.0406349848212\n",
      "Gradient Descent(333/499): loss=6.943217875461991, w0=69.61386138613861, w1=12.134682914533727\n",
      "Gradient Descent(334/499): loss=6.882742244264326, w0=69.69306930693068, w1=12.228730844246254\n",
      "Gradient Descent(335/499): loss=6.8222666130666605, w0=69.77227722772275, w1=12.322778773958781\n",
      "Gradient Descent(336/499): loss=6.761923105549748, w0=69.85643564356434, w1=12.409811735727686\n",
      "Gradient Descent(337/499): loss=6.703293603984816, w0=69.94059405940592, w1=12.496844697496591\n",
      "Gradient Descent(338/499): loss=6.644664102419885, w0=70.02475247524751, w1=12.583877659265497\n",
      "Gradient Descent(339/499): loss=6.586969748470706, w0=70.10643564356434, w1=12.668586778378124\n",
      "Gradient Descent(340/499): loss=6.531604978794614, w0=70.18564356435641, w1=12.75460519059729\n",
      "Gradient Descent(341/499): loss=6.4769127309669745, w0=70.26485148514848, w1=12.840623602816457\n",
      "Gradient Descent(342/499): loss=6.422220483139333, w0=70.34405940594056, w1=12.926642015035624\n",
      "Gradient Descent(343/499): loss=6.367528235311693, w0=70.42326732673263, w1=13.01266042725479\n",
      "Gradient Descent(344/499): loss=6.312892495847405, w0=70.49999999999996, w1=13.097423438209706\n",
      "Gradient Descent(345/499): loss=6.260601811155832, w0=70.57673267326729, w1=13.182186449164622\n",
      "Gradient Descent(346/499): loss=6.208311126464256, w0=70.65346534653462, w1=13.266949460119537\n",
      "Gradient Descent(347/499): loss=6.156476283055092, w0=70.72524752475243, w1=13.346395784879897\n",
      "Gradient Descent(348/499): loss=6.110618684544599, w0=70.79702970297025, w1=13.425842109640257\n",
      "Gradient Descent(349/499): loss=6.064761086034106, w0=70.86881188118807, w1=13.505288434400617\n",
      "Gradient Descent(350/499): loss=6.019091247659324, w0=70.93811881188114, w1=13.58298219105465\n",
      "Gradient Descent(351/499): loss=5.975732165798885, w0=71.00742574257421, w1=13.660675947708684\n",
      "Gradient Descent(352/499): loss=5.932373083938442, w0=71.07673267326729, w1=13.738369704362718\n",
      "Gradient Descent(353/499): loss=5.889442357595591, w0=71.1435643564356, w1=13.813584910813658\n",
      "Gradient Descent(354/499): loss=5.848947152969321, w0=71.21039603960392, w1=13.888800117264598\n",
      "Gradient Descent(355/499): loss=5.808451948343048, w0=71.27722772277224, w1=13.964015323715538\n",
      "Gradient Descent(356/499): loss=5.767956743716779, w0=71.34405940594056, w1=14.039230530166478\n",
      "Gradient Descent(357/499): loss=5.729445535754138, w0=71.40594059405937, w1=14.110687500718347\n",
      "Gradient Descent(358/499): loss=5.6941913465635405, w0=71.46287128712868, w1=14.176983495505382\n",
      "Gradient Descent(359/499): loss=5.663646295610925, w0=71.51980198019798, w1=14.243279490292418\n",
      "Gradient Descent(360/499): loss=5.633101244658309, w0=71.57673267326729, w1=14.309575485079453\n",
      "Gradient Descent(361/499): loss=5.604799318067947, w0=71.62376237623758, w1=14.369257823700574\n",
      "Gradient Descent(362/499): loss=5.5817042200489055, w0=71.67079207920787, w1=14.428940162321695\n",
      "Gradient Descent(363/499): loss=5.558609122029865, w0=71.71782178217816, w1=14.488622500942816\n",
      "Gradient Descent(364/499): loss=5.536294365877075, w0=71.76237623762371, w1=14.544472182574221\n",
      "Gradient Descent(365/499): loss=5.516492186654387, w0=71.80198019801975, w1=14.596236092566132\n",
      "Gradient Descent(366/499): loss=5.500536070261963, w0=71.83663366336629, w1=14.644421312118912\n",
      "Gradient Descent(367/499): loss=5.488335398746288, w0=71.86633663366331, w1=14.683523194736889\n",
      "Gradient Descent(368/499): loss=5.478690504071343, w0=71.89603960396033, w1=14.722625077354866\n",
      "Gradient Descent(369/499): loss=5.469045609396399, w0=71.92574257425736, w1=14.761726959972844\n",
      "Gradient Descent(370/499): loss=5.459400714721454, w0=71.95544554455438, w1=14.800828842590821\n",
      "Gradient Descent(371/499): loss=5.44980772629457, w0=71.98267326732666, w1=14.838910579294522\n",
      "Gradient Descent(372/499): loss=5.441041456063634, w0=72.00990099009894, w1=14.876992315998223\n",
      "Gradient Descent(373/499): loss=5.432275185832697, w0=72.03712871287122, w1=14.915074052701923\n",
      "Gradient Descent(374/499): loss=5.423964656012, w0=72.06188118811875, w1=14.949414929056163\n",
      "Gradient Descent(375/499): loss=5.416796732733374, w0=72.08663366336627, w1=14.983755805410402\n",
      "Gradient Descent(376/499): loss=5.409628809454747, w0=72.1113861386138, w1=15.018096681764641\n",
      "Gradient Descent(377/499): loss=5.402460886176122, w0=72.13613861386132, w1=15.05243755811888\n",
      "Gradient Descent(378/499): loss=5.395292962897496, w0=72.16089108910884, w1=15.08677843447312\n",
      "Gradient Descent(379/499): loss=5.3881250396188705, w0=72.18564356435637, w1=15.121119310827359\n",
      "Gradient Descent(380/499): loss=5.381854026934265, w0=72.2054455445544, w1=15.150225882382873\n",
      "Gradient Descent(381/499): loss=5.376896783224349, w0=72.22524752475242, w1=15.179332453938388\n",
      "Gradient Descent(382/499): loss=5.3719395395144325, w0=72.24504950495044, w1=15.208439025493902\n",
      "Gradient Descent(383/499): loss=5.366982295804515, w0=72.26485148514847, w1=15.237545597049417\n",
      "Gradient Descent(384/499): loss=5.36229905338075, w0=72.28217821782174, w1=15.264462870882356\n",
      "Gradient Descent(385/499): loss=5.358200032197837, w0=72.299504950495, w1=15.291380144715296\n",
      "Gradient Descent(386/499): loss=5.354101011014924, w0=72.31683168316827, w1=15.318297418548235\n",
      "Gradient Descent(387/499): loss=5.35000198983201, w0=72.33415841584154, w1=15.345214692381175\n",
      "Gradient Descent(388/499): loss=5.3459029686490975, w0=72.35148514851481, w1=15.372131966214114\n",
      "Gradient Descent(389/499): loss=5.342125969343629, w0=72.36633663366332, w1=15.396074734501704\n",
      "Gradient Descent(390/499): loss=5.33895067828607, w0=72.38118811881183, w1=15.420017502789294\n",
      "Gradient Descent(391/499): loss=5.33577538722851, w0=72.39603960396035, w1=15.443960271076884\n",
      "Gradient Descent(392/499): loss=5.333303098282137, w0=72.40594059405936, w1=15.461347235032985\n",
      "Gradient Descent(393/499): loss=5.331701753799931, w0=72.41584158415837, w1=15.478734198989086\n",
      "Gradient Descent(394/499): loss=5.330179352532108, w0=72.42821782178214, w1=15.493348062775317\n",
      "Gradient Descent(395/499): loss=5.328712407442178, w0=72.44059405940591, w1=15.507961926561547\n",
      "Gradient Descent(396/499): loss=5.327245462352249, w0=72.45297029702968, w1=15.522575790347778\n",
      "Gradient Descent(397/499): loss=5.32577851726232, w0=72.46534653465345, w1=15.537189654134009\n",
      "Gradient Descent(398/499): loss=5.324342622441918, w0=72.47524752475246, w1=15.552527823153179\n",
      "Gradient Descent(399/499): loss=5.323023530582282, w0=72.48267326732672, w1=15.567055802312778\n",
      "Gradient Descent(400/499): loss=5.322001728347936, w0=72.48762376237622, w1=15.57999729265065\n",
      "Gradient Descent(401/499): loss=5.321233770054335, w0=72.49257425742572, w1=15.59293878298852\n",
      "Gradient Descent(402/499): loss=5.320578554155795, w0=72.49999999999997, w1=15.602543970343588\n",
      "Gradient Descent(403/499): loss=5.319988949048175, w0=72.50742574257423, w1=15.612149157698655\n",
      "Gradient Descent(404/499): loss=5.319408807862436, w0=72.51732673267324, w1=15.618566112152687\n",
      "Gradient Descent(405/499): loss=5.318858710926612, w0=72.5247524752475, w1=15.628171299507754\n",
      "Gradient Descent(406/499): loss=5.318311347210871, w0=72.53465346534651, w1=15.634588253961786\n",
      "Gradient Descent(407/499): loss=5.317830563753273, w0=72.54207920792076, w1=15.63913796720114\n",
      "Gradient Descent(408/499): loss=5.317527197579915, w0=72.54950495049502, w1=15.643687680440495\n",
      "Gradient Descent(409/499): loss=5.317228784682252, w0=72.55445544554452, w1=15.651425626580885\n",
      "Gradient Descent(410/499): loss=5.316935965255146, w0=72.56188118811878, w1=15.65597533982024\n",
      "Gradient Descent(411/499): loss=5.316632599081787, w0=72.56930693069303, w1=15.660525053059594\n",
      "Gradient Descent(412/499): loss=5.316329232908428, w0=72.57673267326729, w1=15.665074766298948\n",
      "Gradient Descent(413/499): loss=5.316027653381187, w0=72.58168316831679, w1=15.672812712439338\n",
      "Gradient Descent(414/499): loss=5.315738000583659, w0=72.58910891089104, w1=15.677362425678693\n",
      "Gradient Descent(415/499): loss=5.315434634410302, w0=72.5965346534653, w1=15.681912138918047\n",
      "Gradient Descent(416/499): loss=5.315131268236943, w0=72.60396039603955, w1=15.686461852157402\n",
      "Gradient Descent(417/499): loss=5.314827902063585, w0=72.61138613861381, w1=15.691011565396757\n",
      "Gradient Descent(418/499): loss=5.314538655928712, w0=72.61633663366331, w1=15.698749511537146\n",
      "Gradient Descent(419/499): loss=5.314236669738815, w0=72.62376237623756, w1=15.7032992247765\n",
      "Gradient Descent(420/499): loss=5.313933303565457, w0=72.63118811881182, w1=15.707848938015855\n",
      "Gradient Descent(421/499): loss=5.3136299373921, w0=72.63861386138608, w1=15.71239865125521\n",
      "Gradient Descent(422/499): loss=5.313337524627647, w0=72.64356435643558, w1=15.7201365973956\n",
      "Gradient Descent(423/499): loss=5.31303870506733, w0=72.65099009900983, w1=15.724686310634954\n",
      "Gradient Descent(424/499): loss=5.312763545536389, w0=72.65594059405933, w1=15.72905158135122\n",
      "Gradient Descent(425/499): loss=5.3125892935777435, w0=72.66089108910883, w1=15.733416852067485\n",
      "Gradient Descent(426/499): loss=5.3124393919005914, w0=72.66336633663359, w1=15.737392094376887\n",
      "Gradient Descent(427/499): loss=5.312351674293682, w0=72.66584158415834, w1=15.741367336686288\n",
      "Gradient Descent(428/499): loss=5.312263956686773, w0=72.6683168316831, w1=15.74534257899569\n",
      "Gradient Descent(429/499): loss=5.3121762390798635, w0=72.67079207920786, w1=15.749317821305091\n",
      "Gradient Descent(430/499): loss=5.312088521472955, w0=72.67326732673261, w1=15.753293063614493\n",
      "Gradient Descent(431/499): loss=5.312000803866046, w0=72.67574257425737, w1=15.757268305923894\n",
      "Gradient Descent(432/499): loss=5.311913086259137, w0=72.67821782178213, w1=15.761243548233296\n",
      "Gradient Descent(433/499): loss=5.311825368652228, w0=72.68069306930688, w1=15.765218790542697\n",
      "Gradient Descent(434/499): loss=5.311737651045317, w0=72.68316831683164, w1=15.769194032852099\n",
      "Gradient Descent(435/499): loss=5.311690215033612, w0=72.68316831683164, w1=15.77060555454594\n",
      "Gradient Descent(436/499): loss=5.311682245459642, w0=72.68316831683164, w1=15.77201707623978\n",
      "Gradient Descent(437/499): loss=5.311674275885673, w0=72.68316831683164, w1=15.773428597933622\n",
      "Gradient Descent(438/499): loss=5.311666306311705, w0=72.68316831683164, w1=15.774840119627463\n",
      "Gradient Descent(439/499): loss=5.311658336737735, w0=72.68316831683164, w1=15.776251641321304\n",
      "Gradient Descent(440/499): loss=5.311650367163768, w0=72.68316831683164, w1=15.777663163015145\n",
      "Gradient Descent(441/499): loss=5.3116423975897975, w0=72.68316831683164, w1=15.779074684708986\n",
      "Gradient Descent(442/499): loss=5.31163442801583, w0=72.68316831683164, w1=15.780486206402827\n",
      "Gradient Descent(443/499): loss=5.311626458441861, w0=72.68316831683164, w1=15.781897728096668\n",
      "Gradient Descent(444/499): loss=5.311618488867893, w0=72.68316831683164, w1=15.78330924979051\n",
      "Gradient Descent(445/499): loss=5.311610519293924, w0=72.68316831683164, w1=15.78472077148435\n",
      "Gradient Descent(446/499): loss=5.3116025497199555, w0=72.68316831683164, w1=15.786132293178191\n",
      "Gradient Descent(447/499): loss=5.311594580145986, w0=72.68316831683164, w1=15.787543814872032\n",
      "Gradient Descent(448/499): loss=5.311586610572017, w0=72.68316831683164, w1=15.788955336565873\n",
      "Gradient Descent(449/499): loss=5.311578640998049, w0=72.68316831683164, w1=15.790366858259715\n",
      "Gradient Descent(450/499): loss=5.31157067142408, w0=72.68316831683164, w1=15.791778379953556\n",
      "Gradient Descent(451/499): loss=5.311562701850112, w0=72.68316831683164, w1=15.793189901647397\n",
      "Gradient Descent(452/499): loss=5.311554732276143, w0=72.68316831683164, w1=15.794601423341238\n",
      "Gradient Descent(453/499): loss=5.311546762702174, w0=72.68316831683164, w1=15.796012945035079\n",
      "Gradient Descent(454/499): loss=5.311538793128206, w0=72.68316831683164, w1=15.79742446672892\n",
      "Gradient Descent(455/499): loss=5.311530823554236, w0=72.68316831683164, w1=15.798835988422761\n",
      "Gradient Descent(456/499): loss=5.311522853980268, w0=72.68316831683164, w1=15.800247510116602\n",
      "Gradient Descent(457/499): loss=5.311514884406299, w0=72.68316831683164, w1=15.801659031810443\n",
      "Gradient Descent(458/499): loss=5.3115069148323295, w0=72.68316831683164, w1=15.803070553504284\n",
      "Gradient Descent(459/499): loss=5.311498945258362, w0=72.68316831683164, w1=15.804482075198125\n",
      "Gradient Descent(460/499): loss=5.311490975684393, w0=72.68316831683164, w1=15.805893596891966\n",
      "Gradient Descent(461/499): loss=5.311483006110424, w0=72.68316831683164, w1=15.807305118585807\n",
      "Gradient Descent(462/499): loss=5.311475036536455, w0=72.68316831683164, w1=15.808716640279648\n",
      "Gradient Descent(463/499): loss=5.311467066962487, w0=72.68316831683164, w1=15.81012816197349\n",
      "Gradient Descent(464/499): loss=5.311459097388517, w0=72.68316831683164, w1=15.81153968366733\n",
      "Gradient Descent(465/499): loss=5.311451127814548, w0=72.68316831683164, w1=15.812951205361172\n",
      "Gradient Descent(466/499): loss=5.311443158240581, w0=72.68316831683164, w1=15.814362727055013\n",
      "Gradient Descent(467/499): loss=5.311435188666612, w0=72.68316831683164, w1=15.815774248748854\n",
      "Gradient Descent(468/499): loss=5.311427219092643, w0=72.68316831683164, w1=15.817185770442695\n",
      "Gradient Descent(469/499): loss=5.311419249518675, w0=72.68316831683164, w1=15.818597292136536\n",
      "Gradient Descent(470/499): loss=5.311411279944705, w0=72.68316831683164, w1=15.820008813830377\n",
      "Gradient Descent(471/499): loss=5.3114033103707365, w0=72.68316831683164, w1=15.821420335524218\n",
      "Gradient Descent(472/499): loss=5.311395340796768, w0=72.68316831683164, w1=15.822831857218059\n",
      "Gradient Descent(473/499): loss=5.311387371222799, w0=72.68316831683164, w1=15.8242433789119\n",
      "Gradient Descent(474/499): loss=5.311379401648831, w0=72.68316831683164, w1=15.825654900605741\n",
      "Gradient Descent(475/499): loss=5.311371432074861, w0=72.68316831683164, w1=15.827066422299582\n",
      "Gradient Descent(476/499): loss=5.311363462500894, w0=72.68316831683164, w1=15.828477943993423\n",
      "Gradient Descent(477/499): loss=5.311355492926925, w0=72.68316831683164, w1=15.829889465687264\n",
      "Gradient Descent(478/499): loss=5.311347523352955, w0=72.68316831683164, w1=15.831300987381105\n",
      "Gradient Descent(479/499): loss=5.3113395537789865, w0=72.68316831683164, w1=15.832712509074947\n",
      "Gradient Descent(480/499): loss=5.311331584205019, w0=72.68316831683164, w1=15.834124030768788\n",
      "Gradient Descent(481/499): loss=5.31132361463105, w0=72.68316831683164, w1=15.835535552462629\n",
      "Gradient Descent(482/499): loss=5.311315645057081, w0=72.68316831683164, w1=15.83694707415647\n",
      "Gradient Descent(483/499): loss=5.3113076754831114, w0=72.68316831683164, w1=15.83835859585031\n",
      "Gradient Descent(484/499): loss=5.311299705909143, w0=72.68316831683164, w1=15.839770117544152\n",
      "Gradient Descent(485/499): loss=5.311291736335173, w0=72.68316831683164, w1=15.841181639237993\n",
      "Gradient Descent(486/499): loss=5.311283766761205, w0=72.68316831683164, w1=15.842593160931834\n",
      "Gradient Descent(487/499): loss=5.311275797187236, w0=72.68316831683164, w1=15.844004682625675\n",
      "Gradient Descent(488/499): loss=5.3112678276132685, w0=72.68316831683164, w1=15.845416204319516\n",
      "Gradient Descent(489/499): loss=5.311259858039299, w0=72.68316831683164, w1=15.846827726013357\n",
      "Gradient Descent(490/499): loss=5.31125188846533, w0=72.68316831683164, w1=15.848239247707198\n",
      "Gradient Descent(491/499): loss=5.311243918891363, w0=72.68316831683164, w1=15.84965076940104\n",
      "Gradient Descent(492/499): loss=5.3112359493173935, w0=72.68316831683164, w1=15.85106229109488\n",
      "Gradient Descent(493/499): loss=5.311227979743425, w0=72.68316831683164, w1=15.852473812788721\n",
      "Gradient Descent(494/499): loss=5.311220010169455, w0=72.68316831683164, w1=15.853885334482563\n",
      "Gradient Descent(495/499): loss=5.311212040595486, w0=72.68316831683164, w1=15.855296856176404\n",
      "Gradient Descent(496/499): loss=5.3112040710215185, w0=72.68316831683164, w1=15.856708377870245\n",
      "Gradient Descent(497/499): loss=5.31119610144755, w0=72.68316831683164, w1=15.858119899564086\n",
      "Gradient Descent(498/499): loss=5.311188131873581, w0=72.68316831683164, w1=15.859531421257927\n",
      "Gradient Descent(499/499): loss=5.311180162299612, w0=72.68316831683164, w1=15.860942942951768\n",
      "Gradient Descent: execution time=0.092 seconds\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "# Time Visualization\r\n",
    "from ipywidgets import IntSlider, interact\r\n",
    "\r\n",
    "def plot_figure(n_iter):\r\n",
    "    fig = gradient_descent_visualization(\r\n",
    "        gradient_losses, gradient_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\r\n",
    "    fig.set_size_inches(10.0, 6.0)\r\n",
    "\r\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1ae3faee0e3b4e6d80438e374e597e3a"
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4 Stochastic gradient descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "def compute_stoch_gradient(y, tx, w):\r\n",
    "    \"\"\"Compute a stochastic gradient from just few examples n and their corresponding y_n labels.\"\"\"\r\n",
    "    # ***************************************************\r\n",
    "    return compute_gradient(y,tx,w)\r\n",
    "\r\n",
    "\r\n",
    "def stochastic_gradient_descent(\r\n",
    "        y, tx, initial_w, batch_size, max_iters, gamma):\r\n",
    "    \"\"\"Stochastic gradient descent algorithm.\"\"\"\r\n",
    "    # ***************************************************\r\n",
    "    #   Define parameters to store w and loss\r\n",
    "    ws = [initial_w]\r\n",
    "    losses = []\r\n",
    "    w = initial_w\r\n",
    "    for n_iter in range(max_iters):\r\n",
    "        for batch_y, batch_x in batch_iter(y, tx, batch_size=batch_size):\r\n",
    "            loss = compute_loss(y, tx, w)\r\n",
    "            gradient = compute_stoch_gradient(batch_y, batch_x, w)\r\n",
    "            w = w - gamma * gradient\r\n",
    "\r\n",
    "        ws.append(w)\r\n",
    "        losses.append(loss)\r\n",
    "        print(\"Stochastic Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\r\n",
    "            bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\r\n",
    "\r\n",
    "    return losses, ws"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "# from stochastic_gradient_descent import *\r\n",
    "\r\n",
    "# Define the parameters of the algorithm.\r\n",
    "max_iters = 500\r\n",
    "gamma = 0.25\r\n",
    "batch_size = 1\r\n",
    "\r\n",
    "# Initialization\r\n",
    "w_initial = np.array([0, 0])\r\n",
    "\r\n",
    "# Start SGD.\r\n",
    "start_time = datetime.datetime.now()\r\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\r\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma)\r\n",
    "end_time = datetime.datetime.now()\r\n",
    "\r\n",
    "# Print result\r\n",
    "exection_time = (end_time - start_time).total_seconds()\r\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stochastic Gradient Descent(0/499): loss=74.06780585492638, w0=0.25, w1=-0.028862251960253948\n",
      "Stochastic Gradient Descent(1/499): loss=73.81780585492638, w0=0.5, w1=-0.3212402411765497\n",
      "Stochastic Gradient Descent(2/499): loss=73.56780585492638, w0=0.75, w1=-0.09582404832124522\n",
      "Stochastic Gradient Descent(3/499): loss=73.31780585492638, w0=1.0, w1=-0.39633065893765407\n",
      "Stochastic Gradient Descent(4/499): loss=73.06780585492638, w0=1.25, w1=-0.4356785578236328\n",
      "Stochastic Gradient Descent(5/499): loss=72.81780585492638, w0=1.5, w1=-0.8062193413284687\n",
      "Stochastic Gradient Descent(6/499): loss=72.56780585492638, w0=1.75, w1=-0.736096272215173\n",
      "Stochastic Gradient Descent(7/499): loss=72.31780585492638, w0=2.0, w1=-0.6367652119288559\n",
      "Stochastic Gradient Descent(8/499): loss=72.06780585492638, w0=2.25, w1=-0.42522308290450656\n",
      "Stochastic Gradient Descent(9/499): loss=71.81780585492638, w0=2.5, w1=-0.23801574310032148\n",
      "Stochastic Gradient Descent(10/499): loss=71.56780585492638, w0=2.75, w1=-0.47261608872637995\n",
      "Stochastic Gradient Descent(11/499): loss=71.31780585492636, w0=3.0, w1=-0.45250551192448585\n",
      "Stochastic Gradient Descent(12/499): loss=71.06780585492638, w0=3.25, w1=-0.6611885610480185\n",
      "Stochastic Gradient Descent(13/499): loss=70.81780585492638, w0=3.5, w1=-0.2740902050868392\n",
      "Stochastic Gradient Descent(14/499): loss=70.56780585492636, w0=3.75, w1=-0.39736773641719575\n",
      "Stochastic Gradient Descent(15/499): loss=70.31780585492638, w0=4.0, w1=-1.3867604853061206\n",
      "Stochastic Gradient Descent(16/499): loss=70.06780585492638, w0=4.25, w1=-1.5838931395695774\n",
      "Stochastic Gradient Descent(17/499): loss=69.81780585492638, w0=4.5, w1=-1.033870875859868\n",
      "Stochastic Gradient Descent(18/499): loss=69.56780585492638, w0=4.75, w1=-1.2385173340793765\n",
      "Stochastic Gradient Descent(19/499): loss=69.31780585492638, w0=5.0, w1=-1.0513099942751913\n",
      "Stochastic Gradient Descent(20/499): loss=69.06780585492638, w0=5.25, w1=-1.2737337109967242\n",
      "Stochastic Gradient Descent(21/499): loss=68.81780585492638, w0=5.5, w1=-1.4948555522161466\n",
      "Stochastic Gradient Descent(22/499): loss=68.56780585492638, w0=5.75, w1=-1.1715787296493383\n",
      "Stochastic Gradient Descent(23/499): loss=68.31780585492638, w0=6.0, w1=-1.3423102305158108\n",
      "Stochastic Gradient Descent(24/499): loss=68.06780585492636, w0=6.25, w1=-1.483730317380751\n",
      "Stochastic Gradient Descent(25/499): loss=67.81780585492638, w0=6.5, w1=-1.9065882460928552\n",
      "Stochastic Gradient Descent(26/499): loss=67.56780585492638, w0=6.75, w1=-1.7494723736342186\n",
      "Stochastic Gradient Descent(27/499): loss=67.31780585492638, w0=7.0, w1=-2.0480576798030814\n",
      "Stochastic Gradient Descent(28/499): loss=67.06780585492638, w0=7.25, w1=-2.4372701880435175\n",
      "Stochastic Gradient Descent(29/499): loss=66.81780585492638, w0=7.5, w1=-2.6254691293895345\n",
      "Stochastic Gradient Descent(30/499): loss=66.56780585492638, w0=7.75, w1=-2.7619426953003683\n",
      "Stochastic Gradient Descent(31/499): loss=66.31780585492638, w0=8.0, w1=-3.061952542250301\n",
      "Stochastic Gradient Descent(32/499): loss=66.06780585492638, w0=8.25, w1=-3.0027360209761946\n",
      "Stochastic Gradient Descent(33/499): loss=65.81780585492638, w0=8.5, w1=-2.73631257326641\n",
      "Stochastic Gradient Descent(34/499): loss=65.56780585492638, w0=8.75, w1=-2.933445227529867\n",
      "Stochastic Gradient Descent(35/499): loss=65.31780585492638, w0=9.0, w1=-3.195628914315934\n",
      "Stochastic Gradient Descent(36/499): loss=65.06780585492638, w0=9.25, w1=-3.4468806708840996\n",
      "Stochastic Gradient Descent(37/499): loss=64.81780585492638, w0=9.5, w1=-3.258289308201725\n",
      "Stochastic Gradient Descent(38/499): loss=64.56780585492638, w0=9.75, w1=-3.10560463490979\n",
      "Stochastic Gradient Descent(39/499): loss=64.31780585492638, w0=10.0, w1=-3.2432061135984074\n",
      "Stochastic Gradient Descent(40/499): loss=64.06780585492638, w0=10.25, w1=-3.3401633214493285\n",
      "Stochastic Gradient Descent(41/499): loss=63.81780585492638, w0=10.5, w1=-3.128621192424979\n",
      "Stochastic Gradient Descent(42/499): loss=63.567805854926384, w0=10.75, w1=-3.089228323331839\n",
      "Stochastic Gradient Descent(43/499): loss=63.317805854926384, w0=11.0, w1=-3.240385260750594\n",
      "Stochastic Gradient Descent(44/499): loss=63.067805854926384, w0=11.25, w1=-3.4592417503939554\n",
      "Stochastic Gradient Descent(45/499): loss=62.817805854926384, w0=11.5, w1=-3.693842096020014\n",
      "Stochastic Gradient Descent(46/499): loss=62.567805854926384, w0=11.75, w1=-3.630144909812937\n",
      "Stochastic Gradient Descent(47/499): loss=62.317805854926384, w0=12.0, w1=-3.8813966663811024\n",
      "Stochastic Gradient Descent(48/499): loss=62.06780585492638, w0=12.25, w1=-4.006516999726957\n",
      "Stochastic Gradient Descent(49/499): loss=61.817805854926384, w0=12.5, w1=-3.917374691624393\n",
      "Stochastic Gradient Descent(50/499): loss=61.567805854926384, w0=12.75, w1=-3.9222872616156397\n",
      "Stochastic Gradient Descent(51/499): loss=61.317805854926384, w0=13.0, w1=-3.9021766848137456\n",
      "Stochastic Gradient Descent(52/499): loss=61.067805854926384, w0=13.25, w1=-3.618187497271836\n",
      "Stochastic Gradient Descent(53/499): loss=60.81780585492638, w0=13.5, w1=-3.866967765572108\n",
      "Stochastic Gradient Descent(54/499): loss=60.567805854926384, w0=13.75, w1=-3.992088098917962\n",
      "Stochastic Gradient Descent(55/499): loss=60.317805854926384, w0=14.0, w1=-4.133315524611893\n",
      "Stochastic Gradient Descent(56/499): loss=60.067805854926384, w0=14.25, w1=-3.9025486555450284\n",
      "Stochastic Gradient Descent(57/499): loss=59.817805854926384, w0=14.5, w1=-4.022678283286484\n",
      "Stochastic Gradient Descent(58/499): loss=59.56780585492638, w0=14.75, w1=-4.245826890361822\n",
      "Stochastic Gradient Descent(59/499): loss=59.317805854926384, w0=15.0, w1=-4.026305111386684\n",
      "Stochastic Gradient Descent(60/499): loss=59.067805854926384, w0=15.25, w1=-3.975115614745785\n",
      "Stochastic Gradient Descent(61/499): loss=58.817805854926384, w0=15.5, w1=-3.661125503234783\n",
      "Stochastic Gradient Descent(62/499): loss=58.567805854926384, w0=15.75, w1=-3.5414011977693347\n",
      "Stochastic Gradient Descent(63/499): loss=58.317805854926384, w0=16.0, w1=-3.482310914936254\n",
      "Stochastic Gradient Descent(64/499): loss=58.06780585492639, w0=16.25, w1=-3.5554657434631194\n",
      "Stochastic Gradient Descent(65/499): loss=57.817805854926384, w0=16.5, w1=-4.039021777046415\n",
      "Stochastic Gradient Descent(66/499): loss=57.567805854926384, w0=16.75, w1=-4.14086981081402\n",
      "Stochastic Gradient Descent(67/499): loss=57.317805854926384, w0=17.0, w1=-3.9181954814260993\n",
      "Stochastic Gradient Descent(68/499): loss=57.067805854926384, w0=17.25, w1=-3.7363433943024376\n",
      "Stochastic Gradient Descent(69/499): loss=56.817805854926384, w0=17.5, w1=-3.506725644743939\n",
      "Stochastic Gradient Descent(70/499): loss=56.567805854926384, w0=17.75, w1=-3.0760055412544935\n",
      "Stochastic Gradient Descent(71/499): loss=56.317805854926384, w0=18.0, w1=-3.2280720224297577\n",
      "Stochastic Gradient Descent(72/499): loss=56.067805854926384, w0=18.25, w1=-3.105317514864261\n",
      "Stochastic Gradient Descent(73/499): loss=55.817805854926384, w0=18.5, w1=-2.9386810413047697\n",
      "Stochastic Gradient Descent(74/499): loss=55.567805854926384, w0=18.75, w1=-2.6244917092217417\n",
      "Stochastic Gradient Descent(75/499): loss=55.317805854926384, w0=19.0, w1=-2.500217764082308\n",
      "Stochastic Gradient Descent(76/499): loss=55.067805854926384, w0=19.25, w1=-2.1580266601888054\n",
      "Stochastic Gradient Descent(77/499): loss=54.81780585492638, w0=19.5, w1=-2.3798307936068746\n",
      "Stochastic Gradient Descent(78/499): loss=54.567805854926384, w0=19.75, w1=-2.5048086262784164\n",
      "Stochastic Gradient Descent(79/499): loss=54.317805854926384, w0=20.0, w1=-2.7848917434353178\n",
      "Stochastic Gradient Descent(80/499): loss=54.067805854926384, w0=20.25, w1=-3.0685950807534534\n",
      "Stochastic Gradient Descent(81/499): loss=53.817805854926384, w0=20.5, w1=-3.066299409011187\n",
      "Stochastic Gradient Descent(82/499): loss=53.567805854926384, w0=20.75, w1=-2.8461612129978837\n",
      "Stochastic Gradient Descent(83/499): loss=53.317805854926384, w0=21.0, w1=-2.44676470563664\n",
      "Stochastic Gradient Descent(84/499): loss=53.06780585492638, w0=21.25, w1=-2.3955752089957416\n",
      "Stochastic Gradient Descent(85/499): loss=52.817805854926384, w0=21.5, w1=-2.206983846313367\n",
      "Stochastic Gradient Descent(86/499): loss=52.567805854926384, w0=21.75, w1=-2.019776506509182\n",
      "Stochastic Gradient Descent(87/499): loss=52.317805854926384, w0=22.0, w1=-1.9496534373958863\n",
      "Stochastic Gradient Descent(88/499): loss=52.067805854926384, w0=22.25, w1=-2.3388659456363223\n",
      "Stochastic Gradient Descent(89/499): loss=51.817805854926384, w0=22.5, w1=-2.1059884822640167\n",
      "Stochastic Gradient Descent(90/499): loss=51.567805854926384, w0=22.75, w1=-2.4059983292139493\n",
      "Stochastic Gradient Descent(91/499): loss=51.317805854926384, w0=23.0, w1=-2.309520311253353\n",
      "Stochastic Gradient Descent(92/499): loss=51.06780585492638, w0=23.25, w1=-2.254915397998145\n",
      "Stochastic Gradient Descent(93/499): loss=50.81780585492638, w0=23.5, w1=-1.998428496558435\n",
      "Stochastic Gradient Descent(94/499): loss=50.567805854926384, w0=23.75, w1=-2.0437907984255865\n",
      "Stochastic Gradient Descent(95/499): loss=50.31780585492638, w0=24.0, w1=-1.729800686914584\n",
      "Stochastic Gradient Descent(96/499): loss=50.067805854926384, w0=24.25, w1=-2.030307297530993\n",
      "Stochastic Gradient Descent(97/499): loss=49.81780585492638, w0=24.5, w1=-2.0426968901529468\n",
      "Stochastic Gradient Descent(98/499): loss=49.56780585492638, w0=24.75, w1=-2.217583866366014\n",
      "Stochastic Gradient Descent(99/499): loss=49.317805854926384, w0=25.0, w1=-2.440732473441352\n",
      "Stochastic Gradient Descent(100/499): loss=49.067805854926384, w0=25.25, w1=-2.3179779658758553\n",
      "Stochastic Gradient Descent(101/499): loss=48.81780585492638, w0=25.5, w1=-2.5119124163633906\n",
      "Stochastic Gradient Descent(102/499): loss=48.567805854926384, w0=25.75, w1=-2.754179200640013\n",
      "Stochastic Gradient Descent(103/499): loss=48.317805854926384, w0=26.0, w1=-2.659363592702788\n",
      "Stochastic Gradient Descent(104/499): loss=48.06780585492638, w0=26.25, w1=-2.2815366974072315\n",
      "Stochastic Gradient Descent(105/499): loss=47.81780585492638, w0=26.5, w1=-2.240886580687834\n",
      "Stochastic Gradient Descent(106/499): loss=47.567805854926384, w0=26.75, w1=-2.462690714105903\n",
      "Stochastic Gradient Descent(107/499): loss=47.317805854926384, w0=27.0, w1=-2.367875106168678\n",
      "Stochastic Gradient Descent(108/499): loss=47.067805854926384, w0=27.25, w1=-2.407223005054657\n",
      "Stochastic Gradient Descent(109/499): loss=46.817805854926384, w0=27.5, w1=-2.14580707880193\n",
      "Stochastic Gradient Descent(110/499): loss=46.56780585492638, w0=27.75, w1=-2.4458169257518625\n",
      "Stochastic Gradient Descent(111/499): loss=46.317805854926384, w0=28.0, w1=-2.5113404182698695\n",
      "Stochastic Gradient Descent(112/499): loss=46.067805854926384, w0=28.25, w1=-2.6634068994451336\n",
      "Stochastic Gradient Descent(113/499): loss=45.817805854926384, w0=28.5, w1=-2.872089948568666\n",
      "Stochastic Gradient Descent(114/499): loss=45.567805854926384, w0=28.75, w1=-3.0043285523289507\n",
      "Stochastic Gradient Descent(115/499): loss=45.317805854926384, w0=29.0, w1=-3.141930031017568\n",
      "Stochastic Gradient Descent(116/499): loss=45.067805854926384, w0=29.25, w1=-2.780259194820151\n",
      "Stochastic Gradient Descent(117/499): loss=44.817805854926384, w0=29.5, w1=-2.9676280965029154\n",
      "Stochastic Gradient Descent(118/499): loss=44.567805854926384, w0=29.75, w1=-2.701204648793131\n",
      "Stochastic Gradient Descent(119/499): loss=44.317805854926384, w0=30.0, w1=-2.976498600960341\n",
      "Stochastic Gradient Descent(120/499): loss=44.067805854926384, w0=30.25, w1=-3.3253548434395976\n",
      "Stochastic Gradient Descent(121/499): loss=43.817805854926384, w0=30.5, w1=-3.7145673516800333\n",
      "Stochastic Gradient Descent(122/499): loss=43.567805854926384, w0=30.75, w1=-3.4816898883077276\n",
      "Stochastic Gradient Descent(123/499): loss=43.317805854926384, w0=31.0, w1=-3.104403734243838\n",
      "Stochastic Gradient Descent(124/499): loss=43.067805854926384, w0=31.25, w1=-2.9472878617852016\n",
      "Stochastic Gradient Descent(125/499): loss=42.817805854926384, w0=31.5, w1=-2.976150113745456\n",
      "Stochastic Gradient Descent(126/499): loss=42.567805854926384, w0=31.75, w1=-3.240197542118293\n",
      "Stochastic Gradient Descent(127/499): loss=42.317805854926384, w0=32.0, w1=-3.1765003559112164\n",
      "Stochastic Gradient Descent(128/499): loss=42.067805854926384, w0=32.25, w1=-3.17420468416895\n",
      "Stochastic Gradient Descent(129/499): loss=41.817805854926384, w0=32.5, w1=-3.1832396003792502\n",
      "Stochastic Gradient Descent(130/499): loss=41.567805854926384, w0=32.75, w1=-3.056444072690027\n",
      "Stochastic Gradient Descent(131/499): loss=41.317805854926384, w0=33.0, w1=-2.7424269420128224\n",
      "Stochastic Gradient Descent(132/499): loss=41.067805854926384, w0=33.25, w1=-2.723798247180783\n",
      "Stochastic Gradient Descent(133/499): loss=40.817805854926384, w0=33.5, w1=-2.8237728873366628\n",
      "Stochastic Gradient Descent(134/499): loss=40.567805854926384, w0=33.75, w1=-2.594155137778164\n",
      "Stochastic Gradient Descent(135/499): loss=40.317805854926384, w0=34.0, w1=-2.685380809600154\n",
      "Stochastic Gradient Descent(136/499): loss=40.06780585492638, w0=34.25, w1=-2.3080946555362645\n",
      "Stochastic Gradient Descent(137/499): loss=39.81780585492638, w0=34.5, w1=-2.656950898015521\n",
      "Stochastic Gradient Descent(138/499): loss=39.567805854926384, w0=34.75, w1=-2.4358318280352833\n",
      "Stochastic Gradient Descent(139/499): loss=39.317805854926384, w0=35.0, w1=-2.654688317678645\n",
      "Stochastic Gradient Descent(140/499): loss=39.06780585492638, w0=35.25, w1=-2.690501567407291\n",
      "Stochastic Gradient Descent(141/499): loss=38.817805854926384, w0=35.5, w1=-2.591170507120974\n",
      "Stochastic Gradient Descent(142/499): loss=38.567805854926384, w0=35.75, w1=-2.748202080365683\n",
      "Stochastic Gradient Descent(143/499): loss=38.317805854926384, w0=36.0, w1=-2.4424084901854908\n",
      "Stochastic Gradient Descent(144/499): loss=38.06780585492638, w0=36.25, w1=-2.6854796250626634\n",
      "Stochastic Gradient Descent(145/499): loss=37.81780585492638, w0=36.5, w1=-2.8322921909706014\n",
      "Stochastic Gradient Descent(146/499): loss=37.567805854926384, w0=36.75, w1=-2.518302079459599\n",
      "Stochastic Gradient Descent(147/499): loss=37.317805854926384, w0=37.0, w1=-2.4776519627402016\n",
      "Stochastic Gradient Descent(148/499): loss=37.06780585492638, w0=37.25, w1=-2.5776266028960815\n",
      "Stochastic Gradient Descent(149/499): loss=36.817805854926384, w0=37.5, w1=-2.944388102036272\n",
      "Stochastic Gradient Descent(150/499): loss=36.567805854926384, w0=37.75, w1=-2.9420924302940055\n",
      "Stochastic Gradient Descent(151/499): loss=36.317805854926384, w0=38.0, w1=-3.3126332137988412\n",
      "Stochastic Gradient Descent(152/499): loss=36.067805854926384, w0=38.25, w1=-3.61313982441525\n",
      "Stochastic Gradient Descent(153/499): loss=35.81780585492638, w0=38.5, w1=-3.387327528528932\n",
      "Stochastic Gradient Descent(154/499): loss=35.567805854926384, w0=38.75, w1=-3.5074571562703882\n",
      "Stochastic Gradient Descent(155/499): loss=35.317805854926384, w0=39.0, w1=-3.340820682710897\n",
      "Stochastic Gradient Descent(156/499): loss=35.06780585492639, w0=39.25, w1=-3.6408305296608297\n",
      "Stochastic Gradient Descent(157/499): loss=34.81780585492638, w0=39.5, w1=-3.8830973139374523\n",
      "Stochastic Gradient Descent(158/499): loss=34.567805854926384, w0=39.75, w1=-3.6635755349623143\n",
      "Stochastic Gradient Descent(159/499): loss=34.317805854926384, w0=40.0, w1=-3.8575099854498496\n",
      "Stochastic Gradient Descent(160/499): loss=34.067805854926384, w0=40.25, w1=-3.5435198739388474\n",
      "Stochastic Gradient Descent(161/499): loss=33.81780585492638, w0=40.5, w1=-3.2845840917671314\n",
      "Stochastic Gradient Descent(162/499): loss=33.567805854926384, w0=40.75, w1=-2.853863988277686\n",
      "Stochastic Gradient Descent(163/499): loss=33.317805854926384, w0=41.0, w1=-2.8896772380063322\n",
      "Stochastic Gradient Descent(164/499): loss=33.06780585492638, w0=41.25, w1=-3.014797571352186\n",
      "Stochastic Gradient Descent(165/499): loss=32.81780585492638, w0=41.5, w1=-2.848161097792695\n",
      "Stochastic Gradient Descent(166/499): loss=32.567805854926384, w0=41.75, w1=-2.611323841705892\n",
      "Stochastic Gradient Descent(167/499): loss=32.317805854926384, w0=42.0, w1=-2.376615733421969\n",
      "Stochastic Gradient Descent(168/499): loss=32.067805854926384, w0=42.25, w1=-2.4054779853822232\n",
      "Stochastic Gradient Descent(169/499): loss=31.817805854926373, w0=42.5, w1=-3.6101721559499653\n",
      "Stochastic Gradient Descent(170/499): loss=31.606972795792274, w0=42.75, w1=-3.6007533521553863\n",
      "Stochastic Gradient Descent(171/499): loss=31.36392297745679, w0=43.0, w1=-3.72587368550124\n",
      "Stochastic Gradient Descent(172/499): loss=31.131272727996386, w0=43.25, w1=-3.811937940739329\n",
      "Stochastic Gradient Descent(173/499): loss=30.896960457712456, w0=43.5, w1=-3.9656277937610205\n",
      "Stochastic Gradient Descent(174/499): loss=30.670698291460532, w0=43.75, w1=-3.52301482912602\n",
      "Stochastic Gradient Descent(175/499): loss=30.39733598947807, w0=44.0, w1=-3.10575382894121\n",
      "Stochastic Gradient Descent(176/499): loss=30.12918142092798, w0=44.25, w1=-3.3281775456627427\n",
      "Stochastic Gradient Descent(177/499): loss=29.90404397068219, w0=44.5, w1=-3.4281521858186226\n",
      "Stochastic Gradient Descent(178/499): loss=29.674855281398564, w0=44.75, w1=-3.69219961419146\n",
      "Stochastic Gradient Descent(179/499): loss=29.464006354899603, w0=45.0, w1=-3.9845776034077556\n",
      "Stochastic Gradient Descent(180/499): loss=29.261120371513428, w0=45.25, w1=-3.91359157990114\n",
      "Stochastic Gradient Descent(181/499): loss=29.02115870491273, w0=45.5, w1=-3.61316651982091\n",
      "Stochastic Gradient Descent(182/499): loss=28.756469679448934, w0=45.75, w1=-3.5539499985468037\n",
      "Stochastic Gradient Descent(183/499): loss=28.518182631809477, w0=46.0, w1=-3.2306731759799954\n",
      "Stochastic Gradient Descent(184/499): loss=28.250646219740315, w0=46.25, w1=-3.36291177974028\n",
      "Stochastic Gradient Descent(185/499): loss=28.03516847967861, w0=46.5, w1=-3.2431874742748317\n",
      "Stochastic Gradient Descent(186/499): loss=27.79079561135605, w0=46.75, w1=-3.380788952963449\n",
      "Stochastic Gradient Descent(187/499): loss=27.577004316708447, w0=47.0, w1=-4.585483123531191\n",
      "Stochastic Gradient Descent(188/499): loss=27.54849455622566, w0=47.25, w1=-4.742695714497966\n",
      "Stochastic Gradient Descent(189/499): loss=27.364044130432834, w0=47.5, w1=-4.8749343182582505\n",
      "Stochastic Gradient Descent(190/499): loss=27.178295287117198, w0=47.75, w1=-5.120182402367585\n",
      "Stochastic Gradient Descent(191/499): loss=27.01572494988037, w0=48.0, w1=-4.853758954657801\n",
      "Stochastic Gradient Descent(192/499): loss=26.748209596756432, w0=48.25, w1=-4.7643728416356375\n",
      "Stochastic Gradient Descent(193/499): loss=26.517005010884578, w0=48.5, w1=-4.818107368939612\n",
      "Stochastic Gradient Descent(194/499): loss=26.315683703586778, w0=48.75, w1=-4.512313778759419\n",
      "Stochastic Gradient Descent(195/499): loss=26.039564539220216, w0=49.0, w1=-4.170122674865917\n",
      "Stochastic Gradient Descent(196/499): loss=25.756509064382534, w0=49.25, w1=-4.099136651359301\n",
      "Stochastic Gradient Descent(197/499): loss=25.529567584589184, w0=49.5, w1=-4.164088024064719\n",
      "Stochastic Gradient Descent(198/499): loss=25.335798677184165, w0=49.75, w1=-4.012823290831141\n",
      "Stochastic Gradient Descent(199/499): loss=25.093950008990657, w0=50.0, w1=-3.9197117831947614\n",
      "Stochastic Gradient Descent(200/499): loss=24.866475790601523, w0=50.25, w1=-4.0734016362164525\n",
      "Stochastic Gradient Descent(201/499): loss=24.697031364460496, w0=50.5, w1=-4.200803607057823\n",
      "Stochastic Gradient Descent(202/499): loss=24.522654971503965, w0=50.75, w1=-4.422607740475892\n",
      "Stochastic Gradient Descent(203/499): loss=24.378209778875053, w0=51.0, w1=-4.631290789599425\n",
      "Stochastic Gradient Descent(204/499): loss=24.24348965015557, w0=51.25, w1=-4.819572446294863\n",
      "Stochastic Gradient Descent(205/499): loss=24.111296157489104, w0=51.5, w1=-4.9067248836909885\n",
      "Stochastic Gradient Descent(206/499): loss=23.95472451788001, w0=51.75, w1=-4.72971550495212\n",
      "Stochastic Gradient Descent(207/499): loss=23.7144774743043, w0=51.5, w1=-4.431130198783257\n",
      "Stochastic Gradient Descent(208/499): loss=23.803761741826108, w0=51.75, w1=-4.214914787553263\n",
      "Stochastic Gradient Descent(209/499): loss=23.55027107610711, w0=52.0, w1=-3.9947765915399596\n",
      "Stochastic Gradient Descent(210/499): loss=23.295549464012446, w0=52.25, w1=-4.215898432759382\n",
      "Stochastic Gradient Descent(211/499): loss=23.183614160627137, w0=52.5, w1=-4.258716937576644\n",
      "Stochastic Gradient Descent(212/499): loss=23.02049668985291, w0=52.25, w1=-3.9217503363133863\n",
      "Stochastic Gradient Descent(213/499): loss=23.08760310299112, w0=52.5, w1=-3.5199182076954196\n",
      "Stochastic Gradient Descent(214/499): loss=22.77525461786498, w0=52.75, w1=-3.331326845013045\n",
      "Stochastic Gradient Descent(215/499): loss=22.530877568110807, w0=53.0, w1=-3.069085508051688\n",
      "Stochastic Gradient Descent(216/499): loss=22.263905222032736, w0=52.75, w1=-2.87367040011102\n",
      "Stochastic Gradient Descent(217/499): loss=22.387020645864556, w0=53.0, w1=-2.882229628612124\n",
      "Stochastic Gradient Descent(218/499): loss=22.203864631466395, w0=53.25, w1=-4.086923799179866\n",
      "Stochastic Gradient Descent(219/499): loss=22.441957386792627, w0=53.5, w1=-4.332171883289201\n",
      "Stochastic Gradient Descent(220/499): loss=22.36770534066761, w0=53.75, w1=-4.1719365122946614\n",
      "Stochastic Gradient Descent(221/499): loss=22.141055568925783, w0=54.0, w1=-3.866142922114469\n",
      "Stochastic Gradient Descent(222/499): loss=21.85722984482309, w0=54.25, w1=-3.878532514736423\n",
      "Stochastic Gradient Descent(223/499): loss=21.69874406417934, w0=54.5, w1=-4.032222367758115\n",
      "Stochastic Gradient Descent(224/499): loss=21.596953123610714, w0=54.75, w1=-3.973132084925034\n",
      "Stochastic Gradient Descent(225/499): loss=21.413904641735467, w0=54.5, w1=-3.5502741562129296\n",
      "Stochastic Gradient Descent(226/499): loss=21.40606893210615, w0=54.75, w1=-3.4800884123634646\n",
      "Stochastic Gradient Descent(227/499): loss=21.21505475724219, w0=55.0, w1=-3.4406955432703246\n",
      "Stochastic Gradient Descent(228/499): loss=21.036170630066184, w0=55.25, w1=-3.4090785741670655\n",
      "Stochastic Gradient Descent(229/499): loss=20.86508137945295, w0=55.5, w1=-3.480774837398941\n",
      "Stochastic Gradient Descent(230/499): loss=20.74194310362192, w0=55.75, w1=-3.4874103584342544\n",
      "Stochastic Gradient Descent(231/499): loss=20.593755217407505, w0=56.0, w1=-3.3982680503316907\n",
      "Stochastic Gradient Descent(232/499): loss=20.40579184661197, w0=56.25, w1=-3.551957903353382\n",
      "Stochastic Gradient Descent(233/499): loss=20.33547868478404, w0=56.5, w1=-3.289716566392025\n",
      "Stochastic Gradient Descent(234/499): loss=20.074640768299396, w0=56.75, w1=-3.44087350381078\n",
      "Stochastic Gradient Descent(235/499): loss=20.010917450725614, w0=57.0, w1=-4.645567674378522\n",
      "Stochastic Gradient Descent(236/499): loss=20.474474507902066, w0=57.25, w1=-4.643272002636255\n",
      "Stochastic Gradient Descent(237/499): loss=20.35057118952379, w0=57.5, w1=-4.592082505995356\n",
      "Stochastic Gradient Descent(238/499): loss=20.20545490634651, w0=57.75, w1=-4.719484476836726\n",
      "Stochastic Gradient Descent(239/499): loss=20.15211021814004, w0=57.5, w1=-4.468232720268561\n",
      "Stochastic Gradient Descent(240/499): loss=20.14181305081346, w0=57.75, w1=-4.041540278445551\n",
      "Stochastic Gradient Descent(241/499): loss=19.803739597389384, w0=58.0, w1=-3.821402082432248\n",
      "Stochastic Gradient Descent(242/499): loss=19.57180678356687, w0=58.25, w1=-3.60028301245201\n",
      "Stochastic Gradient Descent(243/499): loss=19.339369934643017, w0=58.5, w1=-4.804977183019752\n",
      "Stochastic Gradient Descent(244/499): loss=19.856879635233916, w0=58.75, w1=-4.930097516365606\n",
      "Stochastic Gradient Descent(245/499): loss=19.815109214609546, w0=59.0, w1=-4.95895976832586\n",
      "Stochastic Gradient Descent(246/499): loss=19.7216860154364, w0=58.75, w1=-4.724359422699801\n",
      "Stochastic Gradient Descent(247/499): loss=19.704708674330785, w0=59.0, w1=-4.684966553606661\n",
      "Stochastic Gradient Descent(248/499): loss=19.574659286459074, w0=59.25, w1=-4.468751142376667\n",
      "Stochastic Gradient Descent(249/499): loss=19.349948552689174, w0=59.0, w1=-4.27161848811321\n",
      "Stochastic Gradient Descent(250/499): loss=19.352853742621463, w0=59.25, w1=-4.391748115854666\n",
      "Stochastic Gradient Descent(251/499): loss=19.308405273487548, w0=59.5, w1=-4.1958929359411625\n",
      "Stochastic Gradient Descent(252/499): loss=19.09792198732596, w0=59.75, w1=-4.26084430864658\n",
      "Stochastic Gradient Descent(253/499): loss=19.033563894490996, w0=60.0, w1=-4.038169979258659\n",
      "Stochastic Gradient Descent(254/499): loss=18.812090359415265, w0=59.75, w1=-3.941212771407738\n",
      "Stochastic Gradient Descent(255/499): loss=18.85777695162137, w0=60.0, w1=-3.62702343932471\n",
      "Stochastic Gradient Descent(256/499): loss=18.585973143477318, w0=59.75, w1=-3.334645450108414\n",
      "Stochastic Gradient Descent(257/499): loss=18.52899587263043, w0=60.0, w1=-3.5220143517911784\n",
      "Stochastic Gradient Descent(258/499): loss=18.52822156292144, w0=60.25, w1=-3.47082485515028\n",
      "Stochastic Gradient Descent(259/499): loss=18.401059104773584, w0=60.5, w1=-3.398525965849923\n",
      "Stochastic Gradient Descent(260/499): loss=18.264357591609887, w0=60.75, w1=-3.328340222000458\n",
      "Stochastic Gradient Descent(261/499): loss=18.13110192818828, w0=61.0, w1=-3.1167980929761083\n",
      "Stochastic Gradient Descent(262/499): loss=17.918903719371922, w0=61.25, w1=-2.9349460058524466\n",
      "Stochastic Gradient Descent(263/499): loss=17.72328634967095, w0=61.0, w1=-2.634936158902514\n",
      "Stochastic Gradient Descent(264/499): loss=17.64980085884289, w0=61.25, w1=-2.8436192080260465\n",
      "Stochastic Gradient Descent(265/499): loss=17.672283560697196, w0=61.0, w1=-2.800800703208785\n",
      "Stochastic Gradient Descent(266/499): loss=17.742430344961093, w0=61.25, w1=-2.581545555573934\n",
      "Stochastic Gradient Descent(267/499): loss=17.525924687839264, w0=61.0, w1=-2.179426836466373\n",
      "Stochastic Gradient Descent(268/499): loss=17.39541500057652, w0=61.25, w1=-1.986227889333822\n",
      "Stochastic Gradient Descent(269/499): loss=17.193460810573928, w0=61.5, w1=-1.9139290000334652\n",
      "Stochastic Gradient Descent(270/499): loss=17.059025030099406, w0=61.75, w1=-1.9676635273374394\n",
      "Stochastic Gradient Descent(271/499): loss=16.99497445860737, w0=62.0, w1=-1.8922476721352237\n",
      "Stochastic Gradient Descent(272/499): loss=16.85879796283444, w0=62.25, w1=-1.9211099240954777\n",
      "Stochastic Gradient Descent(273/499): loss=16.78085710458271, w0=62.5, w1=-1.7052056005761351\n",
      "Stochastic Gradient Descent(274/499): loss=16.566711776594495, w0=62.75, w1=-1.7410188503047814\n",
      "Stochastic Gradient Descent(275/499): loss=16.499156423775247, w0=63.0, w1=-1.8141736788316467\n",
      "Stochastic Gradient Descent(276/499): loss=16.45616924340446, w0=62.75, w1=-1.736581891624593\n",
      "Stochastic Gradient Descent(277/499): loss=16.496636849609118, w0=62.5, w1=-1.5845154104493289\n",
      "Stochastic Gradient Descent(278/499): loss=16.498821558475125, w0=62.75, w1=-1.3341818399369172\n",
      "Stochastic Gradient Descent(279/499): loss=16.2681297014114, w0=63.0, w1=-1.3630440918971711\n",
      "Stochastic Gradient Descent(280/499): loss=16.197885774722135, w0=62.75, w1=-1.1923125910306984\n",
      "Stochastic Gradient Descent(281/499): loss=16.187785486658928, w0=62.5, w1=-0.9734561013873368\n",
      "Stochastic Gradient Descent(282/499): loss=16.15756653002467, w0=62.25, w1=-0.8764988935364157\n",
      "Stochastic Gradient Descent(283/499): loss=16.19747876197911, w0=62.0, w1=-0.6418985479103573\n",
      "Stochastic Gradient Descent(284/499): loss=16.160522165980378, w0=62.25, w1=-0.4847826754517208\n",
      "Stochastic Gradient Descent(285/499): loss=15.978719097958614, w0=62.5, w1=-0.5136449274119748\n",
      "Stochastic Gradient Descent(286/499): loss=15.900778239706884, w0=62.25, w1=-0.3862429565706045\n",
      "Stochastic Gradient Descent(287/499): loss=15.923688147490164, w0=62.0, w1=0.0029695516698313718\n",
      "Stochastic Gradient Descent(288/499): loss=15.801998947465703, w0=61.75, w1=0.23756989729588981\n",
      "Stochastic Gradient Descent(289/499): loss=15.772606752959344, w0=61.5, w1=0.4599936140174228\n",
      "Stochastic Gradient Descent(290/499): loss=15.754163548006698, w0=61.75, w1=0.46546063784318564\n",
      "Stochastic Gradient Descent(291/499): loss=15.64843626674025, w0=62.0, w1=0.5684953751851198\n",
      "Stochastic Gradient Descent(292/499): loss=15.492473824632054, w0=61.75, w1=0.8685052221350523\n",
      "Stochastic Gradient Descent(293/499): loss=15.431514105433296, w0=62.0, w1=1.2106963260285548\n",
      "Stochastic Gradient Descent(294/499): loss=15.14436810400137, w0=61.75, w1=1.4594765943288273\n",
      "Stochastic Gradient Descent(295/499): loss=15.12610562973751, w0=62.0, w1=1.3939531018108204\n",
      "Stochastic Gradient Descent(296/499): loss=15.047375266649547, w0=62.25, w1=1.2665511309694502\n",
      "Stochastic Gradient Descent(297/499): loss=15.012133058824473, w0=62.0, w1=1.3635083388203713\n",
      "Stochastic Gradient Descent(298/499): loss=15.063197840218118, w0=61.75, w1=1.655886328036667\n",
      "Stochastic Gradient Descent(299/499): loss=15.027133692282117, w0=62.0, w1=1.6952791971298067\n",
      "Stochastic Gradient Descent(300/499): loss=14.891987912489453, w0=61.75, w1=1.7971272308974118\n",
      "Stochastic Gradient Descent(301/499): loss=14.955961629980605, w0=62.0, w1=1.8608244171044883\n",
      "Stochastic Gradient Descent(302/499): loss=14.807527636322092, w0=62.25, w1=2.127247864814273\n",
      "Stochastic Gradient Descent(303/499): loss=14.558517624935444, w0=62.5, w1=2.488832985480852\n",
      "Stochastic Gradient Descent(304/499): loss=14.26555301915997, w0=62.75, w1=2.4434706836137003\n",
      "Stochastic Gradient Descent(305/499): loss=14.183096562571682, w0=62.5, w1=2.647027870947981\n",
      "Stochastic Gradient Descent(306/499): loss=14.187262209489719, w0=62.25, w1=2.9307312082661165\n",
      "Stochastic Gradient Descent(307/499): loss=14.17100062355647, w0=62.5, w1=3.373344172901117\n",
      "Stochastic Gradient Descent(308/499): loss=13.843131694913106, w0=62.75, w1=3.230680682763053\n",
      "Stochastic Gradient Descent(309/499): loss=13.790478535317385, w0=63.0, w1=3.383365356054988\n",
      "Stochastic Gradient Descent(310/499): loss=13.602700770426443, w0=63.25, w1=3.1787188978354797\n",
      "Stochastic Gradient Descent(311/499): loss=13.596453598664255, w0=63.5, w1=3.5805510264534464\n",
      "Stochastic Gradient Descent(312/499): loss=13.287802651026821, w0=63.25, w1=3.8558449786206563\n",
      "Stochastic Gradient Descent(313/499): loss=13.260706043228446, w0=63.5, w1=4.169862109297861\n",
      "Stochastic Gradient Descent(314/499): loss=12.99573122682735, w0=63.75, w1=4.096707280770995\n",
      "Stochastic Gradient Descent(315/499): loss=12.922365245228304, w0=64.0, w1=3.969305309929625\n",
      "Stochastic Gradient Descent(316/499): loss=12.879858566360209, w0=63.75, w1=4.371424029037186\n",
      "Stochastic Gradient Descent(317/499): loss=12.786197920873494, w0=64.0, w1=4.441547098150482\n",
      "Stochastic Gradient Descent(318/499): loss=12.642892619853312, w0=64.25, w1=4.287614710740263\n",
      "Stochastic Gradient Descent(319/499): loss=12.613715058060649, w0=64.5, w1=4.440299384032198\n",
      "Stochastic Gradient Descent(320/499): loss=12.432368714764161, w0=64.25, w1=4.732677373248493\n",
      "Stochastic Gradient Descent(321/499): loss=12.39037087608312, w0=64.5, w1=4.999100820958278\n",
      "Stochastic Gradient Descent(322/499): loss=12.150246757604185, w0=64.75, w1=5.485797747883901\n",
      "Stochastic Gradient Descent(323/499): loss=11.805502116237037, w0=64.5, w1=5.5729501852800265\n",
      "Stochastic Gradient Descent(324/499): loss=11.875953178095122, w0=64.25, w1=5.890390122482346\n",
      "Stochastic Gradient Descent(325/499): loss=11.848574988427064, w0=64.5, w1=5.669268281262924\n",
      "Stochastic Gradient Descent(326/499): loss=11.830564291781851, w0=64.75, w1=5.424020197153589\n",
      "Stochastic Gradient Descent(327/499): loss=11.83498618637444, w0=64.5, w1=5.658620542779648\n",
      "Stochastic Gradient Descent(328/499): loss=11.835581926179406, w0=64.25, w1=5.838984018431192\n",
      "Stochastic Gradient Descent(329/499): loss=11.871631884459466, w0=64.5, w1=5.617862177211769\n",
      "Stochastic Gradient Descent(330/499): loss=11.854788875841324, w0=64.75, w1=5.487176201950914\n",
      "Stochastic Gradient Descent(331/499): loss=11.804844232654531, w0=65.0, w1=5.664185580689782\n",
      "Stochastic Gradient Descent(332/499): loss=11.611347235842928, w0=65.25, w1=5.875727709714131\n",
      "Stochastic Gradient Descent(333/499): loss=11.405616919021954, w0=65.5, w1=5.809956003171874\n",
      "Stochastic Gradient Descent(334/499): loss=11.338506427579548, w0=65.25, w1=5.991563545221815\n",
      "Stochastic Gradient Descent(335/499): loss=11.349162499602397, w0=65.5, w1=6.353234381419231\n",
      "Stochastic Gradient Descent(336/499): loss=11.07141137096987, w0=65.75, w1=6.619657829129015\n",
      "Stochastic Gradient Descent(337/499): loss=10.840080557732241, w0=66.0, w1=6.576620489315071\n",
      "Stochastic Gradient Descent(338/499): loss=10.759570334825773, w0=66.25, w1=6.890610600826074\n",
      "Stochastic Gradient Descent(339/499): loss=10.505057158575404, w0=66.5, w1=6.854797351097428\n",
      "Stochastic Gradient Descent(340/499): loss=10.426272342295837, w0=66.25, w1=6.9197487238028454\n",
      "Stochastic Gradient Descent(341/499): loss=10.490856235273734, w0=66.5, w1=7.406445650728469\n",
      "Stochastic Gradient Descent(342/499): loss=10.152738613751533, w0=66.75, w1=7.286316022987013\n",
      "Stochastic Gradient Descent(343/499): loss=10.114287713221483, w0=67.0, w1=7.512128318873332\n",
      "Stochastic Gradient Descent(344/499): loss=9.906188757724703, w0=66.75, w1=7.860984561352589\n",
      "Stochastic Gradient Descent(345/499): loss=9.831427275004536, w0=67.0, w1=8.091751430419453\n",
      "Stochastic Gradient Descent(346/499): loss=9.61955617427736, w0=66.75, w1=8.326351776045511\n",
      "Stochastic Gradient Descent(347/499): loss=9.60854769315517, w0=66.5, w1=8.501238752258578\n",
      "Stochastic Gradient Descent(348/499): loss=9.627706655065202, w0=66.25, w1=8.724387359333916\n",
      "Stochastic Gradient Descent(349/499): loss=9.631084111970537, w0=66.5, w1=8.482120575057293\n",
      "Stochastic Gradient Descent(350/499): loss=9.636815759590686, w0=66.25, w1=8.677535682997961\n",
      "Stochastic Gradient Descent(351/499): loss=9.652597272357772, w0=66.5, w1=8.834651555456597\n",
      "Stochastic Gradient Descent(352/499): loss=9.469997616280379, w0=66.75, w1=9.221749911417776\n",
      "Stochastic Gradient Descent(353/499): loss=9.180933098732163, w0=67.0, w1=9.599576806713332\n",
      "Stochastic Gradient Descent(354/499): loss=8.903186261597554, w0=66.75, w1=9.988789314953767\n",
      "Stochastic Gradient Descent(355/499): loss=8.870905775430053, w0=67.0, w1=9.902725059715678\n",
      "Stochastic Gradient Descent(356/499): loss=8.777721100494123, w0=67.25, w1=10.333445163205123\n",
      "Stochastic Gradient Descent(357/499): loss=8.495803616548484, w0=67.0, w1=10.521644104551141\n",
      "Stochastic Gradient Descent(358/499): loss=8.55050892924706, w0=67.25, w1=10.899470999846697\n",
      "Stochastic Gradient Descent(359/499): loss=8.294679212857021, w0=67.0, w1=11.074357976059764\n",
      "Stochastic Gradient Descent(360/499): loss=8.370786097377009, w0=66.75, w1=11.36673596527606\n",
      "Stochastic Gradient Descent(361/499): loss=8.416587379775926, w0=66.5, w1=11.585592454919421\n",
      "Stochastic Gradient Descent(362/499): loss=8.4967085483267, w0=66.75, w1=11.263580931914865\n",
      "Stochastic Gradient Descent(363/499): loss=8.446435419824654, w0=67.0, w1=11.751212879294725\n",
      "Stochastic Gradient Descent(364/499): loss=8.16969349129736, w0=67.25, w1=11.756679903120487\n",
      "Stochastic Gradient Descent(365/499): loss=8.03281509616008, w0=67.5, w1=11.717332004234509\n",
      "Stochastic Gradient Descent(366/499): loss=7.914908328157944, w0=67.75, w1=11.96766557474692\n",
      "Stochastic Gradient Descent(367/499): loss=7.712647627602762, w0=68.0, w1=12.190339904134841\n",
      "Stochastic Gradient Descent(368/499): loss=7.520563053997522, w0=68.25, w1=12.56762605819873\n",
      "Stochastic Gradient Descent(369/499): loss=7.295257411718649, w0=68.0, w1=12.503928871991654\n",
      "Stochastic Gradient Descent(370/499): loss=7.434698869475689, w0=68.25, w1=12.697127819124205\n",
      "Stochastic Gradient Descent(371/499): loss=7.261690228189219, w0=68.5, w1=12.947461389636617\n",
      "Stochastic Gradient Descent(372/499): loss=7.080503525119107, w0=68.75, w1=12.672167437469406\n",
      "Stochastic Gradient Descent(373/499): loss=7.037734093107987, w0=69.0, w1=13.158864364395031\n",
      "Stochastic Gradient Descent(374/499): loss=6.796968221365239, w0=69.25, w1=12.937742523175608\n",
      "Stochastic Gradient Descent(375/499): loss=6.75080911090797, w0=69.5, w1=13.199158449428335\n",
      "Stochastic Gradient Descent(376/499): loss=6.571557784975906, w0=69.75, w1=13.045468596406645\n",
      "Stochastic Gradient Descent(377/499): loss=6.526306107718238, w0=70.0, w1=12.80022051229731\n",
      "Stochastic Gradient Descent(378/499): loss=6.521752340851757, w0=70.25, w1=12.870406256146776\n",
      "Stochastic Gradient Descent(379/499): loss=6.416678477971884, w0=70.5, w1=13.047415634885645\n",
      "Stochastic Gradient Descent(380/499): loss=6.277557059079772, w0=70.75, w1=13.266937413860784\n",
      "Stochastic Gradient Descent(381/499): loss=6.128762240382041, w0=71.0, w1=13.709550378495784\n",
      "Stochastic Gradient Descent(382/499): loss=5.919242753113475, w0=70.75, w1=13.904965486436453\n",
      "Stochastic Gradient Descent(383/499): loss=5.927896422879704, w0=71.0, w1=13.936582455539712\n",
      "Stochastic Gradient Descent(384/499): loss=5.850320600425475, w0=70.75, w1=14.03749239092765\n",
      "Stochastic Gradient Descent(385/499): loss=5.88836011945995, w0=70.5, w1=14.225691332273668\n",
      "Stochastic Gradient Descent(386/499): loss=5.918490041064235, w0=70.75, w1=13.742135298690371\n",
      "Stochastic Gradient Descent(387/499): loss=5.978423104365787, w0=71.0, w1=13.537488840470862\n",
      "Stochastic Gradient Descent(388/499): loss=5.9727151821727835, w0=71.25, w1=13.694604712929499\n",
      "Stochastic Gradient Descent(389/499): loss=5.856290540724997, w0=71.5, w1=14.116320677465175\n",
      "Stochastic Gradient Descent(390/499): loss=5.671278049659025, w0=71.25, w1=14.0526234912581\n",
      "Stochastic Gradient Descent(391/499): loss=5.749194050492476, w0=71.5, w1=14.08424046036136\n",
      "Stochastic Gradient Descent(392/499): loss=5.679785209282549, w0=71.75, w1=14.093659264155939\n",
      "Stochastic Gradient Descent(393/499): loss=5.623750014468959, w0=71.5, w1=14.281858205501957\n",
      "Stochastic Gradient Descent(394/499): loss=5.627380149275884, w0=71.75, w1=14.242510306615978\n",
      "Stochastic Gradient Descent(395/499): loss=5.58821490118813, w0=71.5, w1=14.485581441493151\n",
      "Stochastic Gradient Descent(396/499): loss=5.5775045708977915, w0=71.75, w1=14.574967554515315\n",
      "Stochastic Gradient Descent(397/499): loss=5.512138539999877, w0=72.0, w1=14.27002245793486\n",
      "Stochastic Gradient Descent(398/499): loss=5.538245050501241, w0=72.25, w1=14.321211954575759\n",
      "Stochastic Gradient Descent(399/499): loss=5.495752899193145, w0=72.0, w1=14.658178555839015\n",
      "Stochastic Gradient Descent(400/499): loss=5.466773800451662, w0=71.75, w1=14.17148162891339\n",
      "Stochastic Gradient Descent(401/499): loss=5.605171531565955, w0=72.0, w1=13.926233544804056\n",
      "Stochastic Gradient Descent(402/499): loss=5.62396718075276, w0=72.25, w1=13.931700568629818\n",
      "Stochastic Gradient Descent(403/499): loss=5.587899048036814, w0=72.5, w1=14.11355265575348\n",
      "Stochastic Gradient Descent(404/499): loss=5.51589238277978, w0=72.75, w1=14.334671725733719\n",
      "Stochastic Gradient Descent(405/499): loss=5.454649852230119, w0=73.0, w1=14.458945670873153\n",
      "Stochastic Gradient Descent(406/499): loss=5.427702903605833, w0=72.75, w1=14.751323660089449\n",
      "Stochastic Gradient Descent(407/499): loss=5.390645333742108, w0=72.5, w1=14.948456314352907\n",
      "Stochastic Gradient Descent(408/499): loss=5.379207457539454, w0=72.25, w1=14.828732008887458\n",
      "Stochastic Gradient Descent(409/499): loss=5.414748536752964, w0=72.5, w1=14.923547616824683\n",
      "Stochastic Gradient Descent(410/499): loss=5.382109056316578, w0=72.75, w1=14.974737113465581\n",
      "Stochastic Gradient Descent(411/499): loss=5.364522247152518, w0=73.0, w1=15.033827396298662\n",
      "Stochastic Gradient Descent(412/499): loss=5.3608611706177225, w0=73.25, w1=14.891163906160598\n",
      "Stochastic Gradient Descent(413/499): loss=5.3834583364128905, w0=73.5, w1=15.312879870696275\n",
      "Stochastic Gradient Descent(414/499): loss=5.375041383097891, w0=73.25, w1=15.51220976489236\n",
      "Stochastic Gradient Descent(415/499): loss=5.346872809903912, w0=73.0, w1=15.812219611842291\n",
      "Stochastic Gradient Descent(416/499): loss=5.323398791369198, w0=72.75, w1=15.792109035040397\n",
      "Stochastic Gradient Descent(417/499): loss=5.312139271186858, w0=72.5, w1=15.4539260844181\n",
      "Stochastic Gradient Descent(418/499): loss=5.329108140769053, w0=72.25, w1=15.531517871625153\n",
      "Stochastic Gradient Descent(419/499): loss=5.338666608224743, w0=72.0, w1=15.154231717561265\n",
      "Stochastic Gradient Descent(420/499): loss=5.395385906081266, w0=72.25, w1=15.343410636869164\n",
      "Stochastic Gradient Descent(421/499): loss=5.3521498470927025, w0=72.5, w1=15.079363208496327\n",
      "Stochastic Gradient Descent(422/499): loss=5.3647335644805185, w0=72.75, w1=14.64671217709412\n",
      "Stochastic Gradient Descent(423/499): loss=5.404542212470632, w0=72.5, w1=14.68953068191138\n",
      "Stochastic Gradient Descent(424/499): loss=5.4121328594009706, w0=72.25, w1=14.869894157562923\n",
      "Stochastic Gradient Descent(425/499): loss=5.409094359720409, w0=72.5, w1=13.880501408673998\n",
      "Stochastic Gradient Descent(426/499): loss=5.570001510508774, w0=72.75, w1=13.79443715343591\n",
      "Stochastic Gradient Descent(427/499): loss=5.567188157712184, w0=73.0, w1=13.782047560813956\n",
      "Stochastic Gradient Descent(428/499): loss=5.553233266188511, w0=72.75, w1=14.184166279921516\n",
      "Stochastic Gradient Descent(429/499): loss=5.481640189367863, w0=73.0, w1=14.278981887858741\n",
      "Stochastic Gradient Descent(430/499): loss=5.452393352696395, w0=72.75, w1=14.476114542122199\n",
      "Stochastic Gradient Descent(431/499): loss=5.430317599037552, w0=72.5, w1=14.518933046939459\n",
      "Stochastic Gradient Descent(432/499): loss=5.438942787762611, w0=72.25, w1=14.47167464467023\n",
      "Stochastic Gradient Descent(433/499): loss=5.469027500147878, w0=72.0, w1=14.733858331456299\n",
      "Stochastic Gradient Descent(434/499): loss=5.455156674337747, w0=71.75, w1=14.915465873506239\n",
      "Stochastic Gradient Descent(435/499): loss=5.456658532960949, w0=71.5, w1=15.110880981446908\n",
      "Stochastic Gradient Descent(436/499): loss=5.471698406411309, w0=71.25, w1=15.399493031819027\n",
      "Stochastic Gradient Descent(437/499): loss=5.493202836224311, w0=71.0, w1=15.540720457512958\n",
      "Stochastic Gradient Descent(438/499): loss=5.547742086379539, w0=71.25, w1=15.697836329971594\n",
      "Stochastic Gradient Descent(439/499): loss=5.478012074870091, w0=71.5, w1=15.39289123339114\n",
      "Stochastic Gradient Descent(440/499): loss=5.443305836368102, w0=71.25, w1=15.692901080341072\n",
      "Stochastic Gradient Descent(441/499): loss=5.478218987443741, w0=71.5, w1=15.828093860111332\n",
      "Stochastic Gradient Descent(442/499): loss=5.418002200620052, w0=71.75, w1=15.405235931399229\n",
      "Stochastic Gradient Descent(443/499): loss=5.403073915147974, w0=72.0, w1=15.501713949359825\n",
      "Stochastic Gradient Descent(444/499): loss=5.3647928488183565, w0=72.25, w1=15.365240383448992\n",
      "Stochastic Gradient Descent(445/499): loss=5.350041602322928, w0=72.5, w1=14.16054621288125\n",
      "Stochastic Gradient Descent(446/499): loss=5.5057575338722895, w0=72.75, w1=14.232845102181606\n",
      "Stochastic Gradient Descent(447/499): loss=5.4729105529272815, w0=73.0, w1=14.594430222848185\n",
      "Stochastic Gradient Descent(448/499): loss=5.410207132462581, w0=73.25, w1=14.754665593842725\n",
      "Stochastic Gradient Descent(449/499): loss=5.39585540006879, w0=73.0, w1=14.895893019536656\n",
      "Stochastic Gradient Descent(450/499): loss=5.373703115214174, w0=72.75, w1=15.050104654637877\n",
      "Stochastic Gradient Descent(451/499): loss=5.356807765429036, w0=72.5, w1=15.241361684218665\n",
      "Stochastic Gradient Descent(452/499): loss=5.347058548015301, w0=72.25, w1=15.341336324374545\n",
      "Stochastic Gradient Descent(453/499): loss=5.352350177337109, w0=72.0, w1=15.294077922105316\n",
      "Stochastic Gradient Descent(454/499): loss=5.380547938281615, w0=71.75, w1=15.394052562261196\n",
      "Stochastic Gradient Descent(455/499): loss=5.404095948033901, w0=71.5, w1=15.459824268803454\n",
      "Stochastic Gradient Descent(456/499): loss=5.437549104165707, w0=71.75, w1=15.51904079007756\n",
      "Stochastic Gradient Descent(457/499): loss=5.393275118584586, w0=72.0, w1=15.170184547598303\n",
      "Stochastic Gradient Descent(458/499): loss=5.393371524608799, w0=71.75, w1=15.27203258136591\n",
      "Stochastic Gradient Descent(459/499): loss=5.415306609808133, w0=72.0, w1=15.33112286419899\n",
      "Stochastic Gradient Descent(460/499): loss=5.377612654807208, w0=72.25, w1=15.427600882159586\n",
      "Stochastic Gradient Descent(461/499): loss=5.344531731984599, w0=72.0, w1=15.211696558640243\n",
      "Stochastic Gradient Descent(462/499): loss=5.388129757114658, w0=72.25, w1=15.007050100420734\n",
      "Stochastic Gradient Descent(463/499): loss=5.3902541386206915, w0=72.0, w1=15.084641887627788\n",
      "Stochastic Gradient Descent(464/499): loss=5.404348333985888, w0=71.75, w1=14.96491758216234\n",
      "Stochastic Gradient Descent(465/499): loss=5.449472451499131, w0=72.0, w1=14.919555280295187\n",
      "Stochastic Gradient Descent(466/499): loss=5.427025209066487, w0=71.75, w1=15.321673999402748\n",
      "Stochastic Gradient Descent(467/499): loss=5.410710526392576, w0=71.5, w1=15.47588563450397\n",
      "Stochastic Gradient Descent(468/499): loss=5.436195500721743, w0=71.25, w1=15.664084575849987\n",
      "Stochastic Gradient Descent(469/499): loss=5.4794271324321855, w0=71.0, w1=15.887233182925325\n",
      "Stochastic Gradient Descent(470/499): loss=5.534480407097761, w0=71.25, w1=15.699864281242562\n",
      "Stochastic Gradient Descent(471/499): loss=5.477927052095535, w0=71.5, w1=15.74712268351179\n",
      "Stochastic Gradient Descent(472/499): loss=5.419857311944979, w0=71.75, w1=15.940321630644341\n",
      "Stochastic Gradient Descent(473/499): loss=5.377112041738465, w0=72.0, w1=15.618310107639784\n",
      "Stochastic Gradient Descent(474/499): loss=5.357564620771103, w0=71.75, w1=15.52374399721451\n",
      "Stochastic Gradient Descent(475/499): loss=5.3929437709637424, w0=71.5, w1=15.648864330560365\n",
      "Stochastic Gradient Descent(476/499): loss=5.423889340623164, w0=71.25, w1=15.585167144353289\n",
      "Stochastic Gradient Descent(477/499): loss=5.482735781452861, w0=71.5, w1=15.688201881695223\n",
      "Stochastic Gradient Descent(478/499): loss=5.422104344771694, w0=71.75, w1=15.739391378336121\n",
      "Stochastic Gradient Descent(479/499): loss=5.378122444616372, w0=72.0, w1=15.798607899610227\n",
      "Stochastic Gradient Descent(480/499): loss=5.350518114075364, w0=71.75, w1=15.898582539766107\n",
      "Stochastic Gradient Descent(481/499): loss=5.376376489224258, w0=72.0, w1=16.29797904712735\n",
      "Stochastic Gradient Descent(482/499): loss=5.354402061318116, w0=72.25, w1=16.316607741959388\n",
      "Stochastic Gradient Descent(483/499): loss=5.332431074377159, w0=72.5, w1=16.27725984307341\n",
      "Stochastic Gradient Descent(484/499): loss=5.317181826392761, w0=72.25, w1=15.859998842888599\n",
      "Stochastic Gradient Descent(485/499): loss=5.329268965816416, w0=72.5, w1=15.947253387070432\n",
      "Stochastic Gradient Descent(486/499): loss=5.314036330819002, w0=72.25, w1=16.247263234020366\n",
      "Stochastic Gradient Descent(487/499): loss=5.329585362914713, w0=72.0, w1=16.2449675622781\n",
      "Stochastic Gradient Descent(488/499): loss=5.352941356246724, w0=72.25, w1=16.41160403583759\n",
      "Stochastic Gradient Descent(489/499): loss=5.336732977885797, w0=72.0, w1=16.59321157788753\n",
      "Stochastic Gradient Descent(490/499): loss=5.369278769844064, w0=72.25, w1=16.668627433089746\n",
      "Stochastic Gradient Descent(491/499): loss=5.353862700629927, w0=72.5, w1=16.266508713982184\n",
      "Stochastic Gradient Descent(492/499): loss=5.3167113484331, w0=72.75, w1=15.982805376664048\n",
      "Stochastic Gradient Descent(493/499): loss=5.3119823040508045, w0=73.0, w1=15.937443074796896\n",
      "Stochastic Gradient Descent(494/499): loss=5.3198650657623325, w0=73.25, w1=15.800969508886062\n",
      "Stochastic Gradient Descent(495/499): loss=5.3406592039642975, w0=73.0, w1=15.538728171924706\n",
      "Stochastic Gradient Descent(496/499): loss=5.332820630303116, w0=73.25, w1=15.402254606013873\n",
      "Stochastic Gradient Descent(497/499): loss=5.350003850712084, w0=73.0, w1=15.176838413158569\n",
      "Stochastic Gradient Descent(498/499): loss=5.351784021280258, w0=72.75, w1=14.904925216508902\n",
      "Stochastic Gradient Descent(499/499): loss=5.37229622352947, w0=72.5, w1=15.294137724749337\n",
      "SGD: execution time=0.143 seconds\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "# Time Visualization\r\n",
    "from ipywidgets import IntSlider, interact\r\n",
    "def plot_figure(n_iter):\r\n",
    "    fig = gradient_descent_visualization(\r\n",
    "        sgd_losses, sgd_ws, grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight, n_iter)\r\n",
    "    fig.set_size_inches(10.0, 6.0)\r\n",
    "\r\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gradient_ws)))"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aecd25c56c2b4622b082048b91675a04"
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5 Effect of Outliers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6 Subgradient Descent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Modify the function `compute_loss(y, tx, w)` for the Mean Absolute Error cost function [here](#compute_loss)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "def compute_subgradient(y, tx, w):\r\n",
    "    e = y - tx.dot(w)\r\n",
    "    grad = -tx.T.dot(np.sign(e)) / len(e)\r\n",
    "    return grad\r\n",
    "\r\n",
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\r\n",
    "    \"\"\"Subgradient descent algorithm.\"\"\"\r\n",
    "    # Define parameters to store w and loss\r\n",
    "    ws = [initial_w]\r\n",
    "    losses = []\r\n",
    "    w = initial_w\r\n",
    "    for n_iter in range(max_iters):\r\n",
    "        loss = compute_loss(y, tx, w)\r\n",
    "        subgradient = compute_subgradient(y, tx, w)\r\n",
    "        w = w - gamma * subgradient\r\n",
    "        ws.append(w)\r\n",
    "        losses.append(loss)\r\n",
    "        print(\"Subgradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\r\n",
    "              bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]))\r\n",
    "\r\n",
    "    return losses, ws"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('ml-epfl': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "f1dd2468fd3358de85da1e9b5fe6f02ef600206b98faa3202f3d0343a75835dc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}